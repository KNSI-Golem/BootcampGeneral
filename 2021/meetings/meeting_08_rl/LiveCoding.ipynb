{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291d2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0f2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc311a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<LunarLander<LunarLander-v2>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e66cd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515497b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd5536",
   "metadata": {},
   "source": [
    "# Show time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52279642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00192852,  1.4072621 ,  0.1953179 , -0.16257307, -0.00222785,\n",
       "       -0.04424242,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34ef2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a66c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "\n",
    "while not done:\n",
    "    _, _, done, _ = sample_env.step(sample_env.action_space.sample())\n",
    "    sample_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ca624d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.07781267, -0.03735193,  0.01773274, -0.48825493,  0.01060515,\n",
       "        -3.9004784 ,  1.        ,  0.        ], dtype=float32),\n",
       " -100,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6700d",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052f0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    rewards = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        obs, reward, done, _ =  env.step(agent.act(obs, explore = False))\n",
    "        rewards += reward\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d45e97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_agent(env, agent):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    rewards = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        obs, reward, done, _ =  env.step(agent.act(obs, explore = False))\n",
    "        rewards += reward\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5796202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(env, outputs, neurons):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(env.observation_space.shape),\n",
    "        tf.keras.layers.Dense(neurons, activation='tanh'),\n",
    "        tf.keras.layers.Dense(neurons, activation='tanh'),\n",
    "        tf.keras.layers.Dense(outputs)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac52738",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_I = 5000\n",
    "TEST_N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4a5b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, env, model, gamma, exploration, lr):\n",
    "        self.env = env\n",
    "        self.obs = self.env.reset()\n",
    "        \n",
    "        self.model = model\n",
    "        self.saved_model = tf.keras.models.clone_model(model)\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.exploration = exploration\n",
    "\n",
    "    def step(self):\n",
    "        action = self.act(self.obs, explore=True)\n",
    "        obs, reward, done, _ = self.env.step(action)\n",
    "        prev_obs = self.obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "        else:\n",
    "            self.obs = obs\n",
    "\n",
    "        return prev_obs, obs, action, reward, done\n",
    "\n",
    "    def act(self, obs, explore):\n",
    "        if explore and np.random.rand() < self.exploration:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            q = self.model(obs.reshape((1, -1))).numpy()\n",
    "            return np.argmax(q)\n",
    "\n",
    "    def learn(self, obs, obs_next, actions, rewards, done):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(obs, obs_next, actions, rewards, done)\n",
    "        \n",
    "        v = self.model.trainable_variables\n",
    "        self.optimizer.minimize(loss, v, tape=tape)\n",
    "        \n",
    "        return loss.numpy().mean()\n",
    "\n",
    "    def loss(self, obs, obs_next, actions, rewards, done):\n",
    "        q = self.model(obs)\n",
    "        q_next = self.model(obs_next) * (1 - done).reshape(-1, 1)\n",
    "        \n",
    "        diffs =\\\n",
    "            tf.stop_gradient(rewards.reshape(-1, 1) + self.gamma * tf.reduce_max(q_next, axis=-1, keepdims=True))\\\n",
    "            - tf.reshape(tf.gather_nd(q, actions.reshape(-1, 1), batch_dims=1), (-1, 1))\n",
    "        \n",
    "        return tf.reduce_mean(diffs ** 2)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.saved_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model.set_weights(self.saved_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "671c053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(sample_env, make_model(sample_env, sample_env.action_space.n, 64), 0.97, 0.05, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2754e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-446.3824781359425"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(sample_env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5841f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_agent(env_name, gamma, exploration, lr, neurons, steps):\n",
    "    test_env = gym.make(env_name)\n",
    "    train_env = gym.make(env_name)\n",
    "    agent = QAgent(train_env, make_model(train_env, train_env.action_space.n, neurons), gamma, exploration, lr)\n",
    "    \n",
    "    best_rewards = -np.inf\n",
    "    total_loss = 0\n",
    "    t = 0\n",
    "    i = 0\n",
    "    \n",
    "    def test():\n",
    "        nonlocal best_rewards\n",
    "        reward_mean = sum(test_agent(test_env, agent) for _ in range(TEST_N)) / TEST_N\n",
    "        print(f'Step: {i}; mean rewards: {reward_mean} mean loss: {total_loss / t}')\n",
    "        if reward_mean > best_rewards:\n",
    "            best_rewards = reward_mean\n",
    "            agent.save_model()\n",
    "\n",
    "    for i in range(steps):\n",
    "        obs, obs_next, action, reward, done = agent.step()\n",
    "        \n",
    "        loss = agent.learn(\n",
    "            obs.reshape((1, -1)),\n",
    "            obs_next.reshape((1, -1)),\n",
    "            np.array([action]),\n",
    "            np.array([reward]),\n",
    "            np.array([done])\n",
    "        )\n",
    "        \n",
    "        total_loss += loss\n",
    "        t += 1\n",
    "        \n",
    "        if i % TEST_I == 0:\n",
    "            test()\n",
    "            total_loss = 0\n",
    "            t = 0\n",
    "\n",
    "    test()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23ec50f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0; mean rewards: -177.17759518167736 mean loss: 0.018093472346663475\n",
      "Step: 5000; mean rewards: -558.6611788753227 mean loss: 111.38982937799896\n",
      "Step: 10000; mean rewards: -91.03338388189124 mean loss: 89.59023281809345\n",
      "Step: 15000; mean rewards: -509.3658352870933 mean loss: 172.99317659972957\n",
      "Step: 20000; mean rewards: -526.6223625931985 mean loss: 186.43246589719314\n",
      "Step: 25000; mean rewards: -105.56304294211853 mean loss: 162.472920534716\n",
      "Step: 30000; mean rewards: -572.1939757566408 mean loss: 140.09841206688267\n",
      "Step: 35000; mean rewards: -109.21074362285876 mean loss: 192.83290152246377\n",
      "Step: 40000; mean rewards: -236.6381625160443 mean loss: 196.18119077835246\n",
      "Step: 45000; mean rewards: -289.5913443539947 mean loss: 162.15580050305823\n",
      "Step: 50000; mean rewards: -536.0540739040664 mean loss: 168.2273138917415\n",
      "Step: 55000; mean rewards: -240.6857538616799 mean loss: 167.3019231625149\n",
      "Step: 60000; mean rewards: -192.52857487252624 mean loss: 168.18432192580292\n",
      "Step: 65000; mean rewards: -160.91153383058378 mean loss: 167.81011024550912\n",
      "Step: 70000; mean rewards: -490.3603578574027 mean loss: 159.8872889700246\n",
      "Step: 74999; mean rewards: -533.4936184066624 mean loss: 159.54018644644896\n"
     ]
    }
   ],
   "source": [
    "agent = train_q_agent('LunarLander-v2', 0.97, 0.05, 0.001, 64, 75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "298f1dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-50.57960776961261"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(sample_env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "375935dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08b486b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, size, observation_space, action_space):\n",
    "        self.obs = np.zeros((size,) + observation_space.shape, dtype=observation_space.dtype)\n",
    "        self.obs_next = np.zeros((size,) + observation_space.shape, dtype=observation_space.dtype)\n",
    "        self.actions = np.zeros((size,) + action_space.shape, dtype=action_space.dtype)\n",
    "        \n",
    "        self.rewards = np.zeros((size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((size,), dtype=np.float32)\n",
    "        \n",
    "        self.size = size\n",
    "        self.cur_size = 0\n",
    "        self.next = 0\n",
    "\n",
    "    def put(self, obs, obs_next, action, reward, done):\n",
    "        self.obs[self.next] = obs\n",
    "        self.obs_next[self.next] = obs_next\n",
    "        self.actions[self.next] = action\n",
    "        self.rewards[self.next] = reward\n",
    "        self.dones[self.next] = done\n",
    "        \n",
    "        self.next = (self.next + 1) % self.size\n",
    "        self.cur_size = min(self.cur_size + 1, self.size)\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        ids = np.random.choice(self.cur_size, size=batch_size)\n",
    "        return self.obs[ids], self.obs_next[ids], self.actions[ids], self.rewards[ids], self.dones[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da126b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_agent(env_name, gamma, exploration, lr, neurons, steps, batch_size, mem_size):\n",
    "    test_env = gym.make(env_name)\n",
    "    train_env = gym.make(env_name)\n",
    "    agent = QAgent(train_env, make_model(train_env, train_env.action_space.n, neurons), gamma, exploration, lr)\n",
    "    \n",
    "    memory = Memory(mem_size, train_env.observation_space, train_env.action_space)\n",
    "    \n",
    "    best_rewards = -np.inf\n",
    "    total_loss = 0\n",
    "    t = 0\n",
    "    i = 0\n",
    "    \n",
    "    def test():\n",
    "        nonlocal best_rewards\n",
    "        reward_mean = sum(test_agent(test_env, agent) for _ in range(TEST_N)) / TEST_N\n",
    "        print(f'Step: {i}; mean rewards: {reward_mean} mean loss: {total_loss / t}')\n",
    "        if reward_mean > best_rewards:\n",
    "            best_rewards = reward_mean\n",
    "            agent.save_model()\n",
    "\n",
    "    for i in range(steps):\n",
    "        obs, obs_next, action, reward, done = agent.step()\n",
    "        memory.put(obs, obs_next, action, reward, done)\n",
    "        \n",
    "        loss = agent.learn(*memory.get(batch_size))\n",
    "        \n",
    "        total_loss += loss\n",
    "        t += 1\n",
    "        \n",
    "        if i % TEST_I == 0:\n",
    "            test()\n",
    "            total_loss = 0\n",
    "            t = 0\n",
    "\n",
    "    test()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43493d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0; mean rewards: -337.71626465031477 mean loss: 2.7995450496673584\n",
      "Step: 5000; mean rewards: 72.49090555055014 mean loss: 39.592219853186606\n",
      "Step: 10000; mean rewards: 8.005744037706513 mean loss: 14.014898508131504\n",
      "Step: 15000; mean rewards: -53.987472826910675 mean loss: 7.9595151462435725\n",
      "Step: 20000; mean rewards: 117.83277603120018 mean loss: 6.8369340963721275\n",
      "Step: 25000; mean rewards: 93.03897114283225 mean loss: 7.572686772716045\n",
      "Step: 30000; mean rewards: 45.401753638273455 mean loss: 8.869809253382682\n",
      "Step: 35000; mean rewards: 127.78036391856662 mean loss: 9.444670363330841\n",
      "Step: 40000; mean rewards: 39.39960036487311 mean loss: 9.127043556249141\n",
      "Step: 45000; mean rewards: 44.698645065746256 mean loss: 9.685314298141003\n",
      "Step: 50000; mean rewards: 11.17170726632415 mean loss: 10.15371111023426\n",
      "Step: 55000; mean rewards: 112.00017803436006 mean loss: 9.886017969548702\n",
      "Step: 60000; mean rewards: 202.0123223733297 mean loss: 10.50576485325098\n",
      "Step: 65000; mean rewards: 146.06102653510766 mean loss: 10.576737821674348\n",
      "Step: 70000; mean rewards: 143.92357112285276 mean loss: 9.536218402552604\n",
      "Step: 74999; mean rewards: -125.57057685291768 mean loss: 9.979364856025366\n"
     ]
    }
   ],
   "source": [
    "agent = train_q_agent('LunarLander-v2', 0.97, 0.05, 0.001, 64, 75000, 100, 75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20209bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.3417547808289"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(sample_env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ba941cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f99c5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10c20571",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df06ecba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8866b8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1. -1.], [1. 1.], (2,), float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3ee3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "723ac69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACMemory:\n",
    "    def __init__(self, size, observation_space, action_space):\n",
    "        self.obs = np.zeros((size,) + observation_space.shape, dtype=observation_space.dtype)\n",
    "        self.obs_next = np.zeros((size,) + observation_space.shape, dtype=observation_space.dtype)\n",
    "        self.actions = np.zeros((size,) + action_space.shape, dtype=action_space.dtype)\n",
    "        \n",
    "        self.rewards = np.zeros((size,), dtype=np.float32)\n",
    "        self.probs = np.zeros((size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((size,), dtype=np.float32)\n",
    "        \n",
    "        self.size = size\n",
    "        self.cur_size = 0\n",
    "        self.next = 0\n",
    "\n",
    "    def put(self, obs, obs_next, action, reward, done, probs):\n",
    "        self.obs[self.next] = obs\n",
    "        self.obs_next[self.next] = obs_next\n",
    "        self.actions[self.next] = action\n",
    "        self.rewards[self.next] = reward\n",
    "        self.dones[self.next] = done\n",
    "        self.probs[self.next] = probs\n",
    "        \n",
    "        self.next = (self.next + 1) % self.size\n",
    "        self.cur_size = min(self.cur_size + 1, self.size)\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        ids = np.random.choice(self.cur_size, size=batch_size)\n",
    "        return (\n",
    "            self.obs[ids], self.obs_next[ids], self.actions[ids],\n",
    "            self.rewards[ids], self.dones[ids], self.probs[ids]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6e2f900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, env, actor_model, critic_model, gamma, sigma, lr, b, beta):\n",
    "        self.env = env\n",
    "        self.obs = self.env.reset()\n",
    "        \n",
    "        self.actor = actor_model\n",
    "        self.critic = critic_model\n",
    "        self.saved_actor = tf.keras.models.clone_model(actor_model)\n",
    "        self.saved_critic = tf.keras.models.clone_model(critic_model)\n",
    "        self.gamma = gamma\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.noise = tfp.distributions.MultivariateNormalDiag(\n",
    "            tf.zeros(self.env.action_space.shape, dtype=tf.float32),\n",
    "            tf.ones(self.env.action_space.shape, dtype=tf.float32) * sigma,\n",
    "        )\n",
    "        self.b = b\n",
    "        self.beta = beta\n",
    "\n",
    "    def step(self):\n",
    "        action, prob = self.act(self.obs, explore=True)\n",
    "        obs, reward, done, _ = self.env.step(action)\n",
    "        prev_obs = self.obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "        else:\n",
    "            self.obs = obs\n",
    "\n",
    "        return prev_obs, obs, action, reward, done, prob\n",
    "\n",
    "    def act(self, obs, explore):\n",
    "        actions = self.actor(obs.reshape(1, -1))[0]\n",
    "        \n",
    "        if explore:\n",
    "            noise = self.noise.sample()\n",
    "            probs = self.noise.prob(noise)\n",
    "            return (actions + noise).numpy(), probs.numpy()\n",
    "        else:\n",
    "            return actions\n",
    "\n",
    "    \n",
    "    def learn(self, obs, obs_next, actions, rewards, done, probs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_loss, actor_loss, bounds_penalty = self.loss(obs, obs_next, actions, rewards, done, probs)\n",
    "                \n",
    "        return np.array([critic_loss.numpy(), actor_loss.numpy(), bounds_penalty.numpy()])\n",
    "    \n",
    "    @tf.function\n",
    "    def loss(self, obs, obs_next, actions, rewards, done, probs):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            v = self.critic(obs)\n",
    "            v_next = self.critic(obs_next)\n",
    "            \n",
    "            cur_actions = self.actor(obs)\n",
    "            cur_log_probs = self.noise.log_prob(actions - cur_actions)\n",
    "            cur_probs = tf.exp(cur_log_probs)\n",
    "            \n",
    "            is_base = tf.reshape(cur_probs / probs, (-1, 1))\n",
    "            is_ = tf.tanh(is_base / self.b) * self.b\n",
    "            \n",
    "            td = tf.reshape(rewards, (-1, 1)) + self.gamma * tf.stop_gradient(v_next) - v\n",
    "            \n",
    "            critic_loss = tf.reduce_mean(td ** 2 * is_)\n",
    "            actor_loss = tf.reduce_mean(-tf.reshape(cur_log_probs, (-1, 1)) * td * tf.stop_gradient(is_))\n",
    "            \n",
    "            bounds_penalty = tf.reduce_mean(\n",
    "                tf.maximum(tf.abs(cur_actions) - tf.reshape(self.env.action_space.high, (1, -1)), 0)\n",
    "                ** 2 * self.beta\n",
    "            )\n",
    "            \n",
    "            actor_loss_with_bounds = actor_loss + bounds_penalty\n",
    "        \n",
    "        self.actor_optimizer.minimize(actor_loss_with_bounds, self.actor.trainable_variables, tape=tape)\n",
    "        self.critic_optimizer.minimize(critic_loss, self.critic.trainable_variables, tape=tape)\n",
    "        \n",
    "        return actor_loss, critic_loss, bounds_penalty\n",
    "\n",
    "    def save_model(self):\n",
    "        self.saved_actor.set_weights(self.actor.get_weights())\n",
    "        self.saved_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "    def load_model(self):\n",
    "        self.actor.set_weights(self.saved_actor.get_weights())\n",
    "        self.critic.set_weights(self.saved_critic.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f710b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ac_agent(env_name, gamma, sigma, lr, b, beta, neurons, steps, batch_size, mem_size):\n",
    "    test_env = gym.make(env_name)\n",
    "    train_env = gym.make(env_name)\n",
    "    agent = ActorCriticAgent(\n",
    "        train_env,\n",
    "        make_model(train_env, train_env.action_space.shape[0], neurons),\n",
    "        make_model(train_env, 1, neurons),\n",
    "        gamma, sigma, lr, b, beta\n",
    "    )\n",
    "    \n",
    "    memory = ACMemory(mem_size, train_env.observation_space, train_env.action_space)\n",
    "    \n",
    "    best_rewards = -np.inf\n",
    "    total_loss = 0\n",
    "    t = 0\n",
    "    i = 0\n",
    "    \n",
    "    def test():\n",
    "        nonlocal best_rewards\n",
    "        reward_mean = sum(test_agent(test_env, agent) for _ in range(TEST_N)) / TEST_N\n",
    "        print(f'Step: {i}; mean rewards: {reward_mean} mean loss: {total_loss / t}')\n",
    "        if reward_mean > best_rewards:\n",
    "            best_rewards = reward_mean\n",
    "            agent.save_model()\n",
    "\n",
    "    for i in range(steps):\n",
    "        obs, obs_next, action, reward, done, probs = agent.step()\n",
    "        memory.put(obs, obs_next, action, reward, done, probs)\n",
    "        \n",
    "        loss = agent.learn(*memory.get(batch_size))\n",
    "        \n",
    "        total_loss += loss\n",
    "        t += 1\n",
    "        \n",
    "        if i % TEST_I == 0:\n",
    "            test()\n",
    "            total_loss = 0\n",
    "            t = 0\n",
    "\n",
    "    test()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff1ac9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0; mean rewards: -388.3604990078528 mean loss: [-0.05058021 12.393817    0.        ]\n",
      "Step: 5000; mean rewards: -147.75382120326506 mean loss: [-0.07964392  5.8693876   0.06297314]\n",
      "Step: 10000; mean rewards: -203.96583217131396 mean loss: [-0.04786463  1.9669648   0.02933767]\n",
      "Step: 15000; mean rewards: 58.715670562866286 mean loss: [-0.0310049   1.1684937   0.02051632]\n",
      "Step: 20000; mean rewards: -128.98516214331724 mean loss: [-0.03323267  0.5207679   0.01062053]\n",
      "Step: 25000; mean rewards: -95.16010696081925 mean loss: [-0.04176664  0.41489357  0.00858743]\n",
      "Step: 30000; mean rewards: -88.0114589685005 mean loss: [-0.03559497  0.45041865  0.00724451]\n",
      "Step: 35000; mean rewards: -26.00342753575557 mean loss: [-0.03597514  0.34646347  0.00603455]\n",
      "Step: 40000; mean rewards: -59.711151495201726 mean loss: [-0.03253345  0.2602508   0.00534563]\n",
      "Step: 45000; mean rewards: 31.811532687978676 mean loss: [-0.02531077  0.25308514  0.00502698]\n",
      "Step: 49999; mean rewards: -71.84593574794485 mean loss: [-0.02044668  0.3028044   0.00506681]\n"
     ]
    }
   ],
   "source": [
    "agent = train_ac_agent('LunarLanderContinuous-v2', 0.97, 0.3, 0.001, 3, 0.1, 64, 50000, 100, 75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7931782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-27.669053611754848"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(sample_env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e72d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
