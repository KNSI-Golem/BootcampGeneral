{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbbd1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, outputs, neurons, out_activation):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(input_shape),\n",
    "        tf.keras.layers.Dense(neurons, activation='tanh'),\n",
    "        tf.keras.layers.Dense(neurons, activation='tanh'),\n",
    "        tf.keras.layers.Dense(outputs, activation=out_activation)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334aa77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, input_shape, neurons, lr):\n",
    "        self.model = make_model(input_shape, 1, neurons, None)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    @tf.function\n",
    "    def advantage(self, obs, obs_last, rewards, done, discount):\n",
    "        v = self.model(obs)\n",
    "        v_last = self.model(tf.reshape(obs_last, (1, -1))) * (1 - tf.cast(done, tf.float32))\n",
    "        v = v[:,0]\n",
    "        v_last = v_last[:,0]\n",
    "        vn = tf.concat([v[1:], v_last], axis=0)\n",
    "\n",
    "        adv = rewards + tf.stop_gradient(discount * vn) - v\n",
    "\n",
    "        return adv\n",
    "\n",
    "    def minimize(self, loss, tape):\n",
    "        self.optimizer.minimize(loss, self.model.trainable_variables, tape=tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a67b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteActor:\n",
    "    def __init__(self, input_shape, actions, neurons, lr):\n",
    "        self.model = make_model(input_shape, actions, neurons, 'softmax')\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    @tf.function\n",
    "    def prob(self, obs, actions):\n",
    "        return self.probs_prob(obs, actions)[1]\n",
    "    \n",
    "    @tf.function\n",
    "    def probs_prob(self, obs, actions):\n",
    "        outs = self.model(obs)\n",
    "        actions = tf.reshape(actions, (-1, 1))\n",
    "        return outs, tf.gather_nd(outs, actions, batch_dims=1)\n",
    "\n",
    "    def act(self, obs, explore):\n",
    "        out = self.model(obs.reshape((1, -1))).numpy()[0]\n",
    "        if explore:\n",
    "            return np.random.choice(out.shape[0], p=out)\n",
    "        else:\n",
    "            return np.argmax(out)\n",
    "\n",
    "    def minimize(self, loss, tape):\n",
    "        self.optimizer.minimize(loss, self.model.trainable_variables, tape=tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_actor(actor, test_env, num, show=False):\n",
    "    rewards_sum = 0\n",
    "    for _ in range(num):\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        \n",
    "        if show:\n",
    "            test_env.render()\n",
    "\n",
    "        while not done:\n",
    "            action = actor.act(obs, explore=False)\n",
    "            obs, reward, done, _ = test_env.step(action)\n",
    "\n",
    "            if show:\n",
    "                test_env.render()\n",
    "\n",
    "            rewards_sum += reward\n",
    "    return rewards_sum / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf089afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705818b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784766f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make('CartPole-v1')\n",
    "test_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_aac(train_env, test_env, max_size, T, neurons, lr, discount):\n",
    "    actor = DiscreteActor(train_env.observation_space.shape, train_env.action_space.n, neurons, lr)\n",
    "    critic = Critic(train_env.observation_space.shape, neurons, lr)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    total_critic_loss = 0\n",
    "    \n",
    "    rewards = test_actor(actor, test_env, 5)\n",
    "    print(0, rewards, sep='\\t')\n",
    "\n",
    "    obs = train_env.reset()\n",
    "    \n",
    "    for step in range(max_size):\n",
    "        action = actor.act(obs, explore=True)\n",
    "        next_obs, reward, done, info = train_env.step(action)\n",
    "        \n",
    "        data.append({'obs': obs, 'action': action, 'reward': reward})\n",
    "        \n",
    "        if len(data) == T or done:\n",
    "            actions = np.array([d['action'] for d in data])\n",
    "            observations = np.array([d['obs'] for d in data], np.float32)\n",
    "            rewards = np.array([d['reward'] for d in data], np.float32)\n",
    "            \n",
    "            is_done = done and not info.get('TimeLimit.truncated', False)\n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                advantage = critic.advantage(observations, next_obs, rewards, is_done, discount)\n",
    "                prob = actor.prob(observations, actions)\n",
    "                \n",
    "                critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "                actor_loss = tf.reduce_mean(-tf.math.log(prob) * advantage)\n",
    "\n",
    "            actor.minimize(actor_loss, tape)\n",
    "            critic.minimize(critic_loss, tape)\n",
    "            \n",
    "            total_critic_loss += critic_loss.numpy()\n",
    "            data = []\n",
    "\n",
    "        if done:\n",
    "            obs = train_env.reset()\n",
    "        else:\n",
    "            obs = next_obs\n",
    "\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            rewards = test_actor(actor, test_env, 5)\n",
    "            print(step, rewards, total_critic_loss / 1000, sep='\\t')\n",
    "            total_critic_loss = 0\n",
    "\n",
    "    return actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219b890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actor = train_agent_aac(train_env, test_env, 100000, 25, 64, 0.001, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae26320",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actor(actor, env, 5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6513a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_spread_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88db849",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_spread_v2.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d842b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space(env.agents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    obs, reward, done, _  = env.last()\n",
    "    action = env.action_space(agent).sample()\n",
    "    env.step(None if done else action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf05730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiagent(actors, env, n, show=False):\n",
    "    rewards = 0\n",
    "    for _ in range(n):\n",
    "        env.reset()\n",
    "        if show:\n",
    "            env.render()\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, done, _  = env.last()\n",
    "            action = actors[agent].act(obs, explore=False)\n",
    "            env.step(None if done else action)\n",
    "            if show:\n",
    "                env.render()\n",
    "            rewards += reward\n",
    "    return rewards / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccfe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_iaac(train_env, test_env, max_size, T, neurons, lr, discount):\n",
    "    train_env.reset()\n",
    "    agents = train_env.agents\n",
    "    actors = {\n",
    "        agent: DiscreteActor(train_env.observation_space(agent).shape, train_env.action_space(agent).n, neurons, lr)\n",
    "        for agent in agents\n",
    "    }\n",
    "    critics = {\n",
    "        agent: Critic(train_env.observation_space(agent).shape, neurons, lr)\n",
    "        for agent in agents\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    total_critic_loss = 0\n",
    "    \n",
    "    rewards = test_multiagent(actors, test_env, 5)\n",
    "    print(0, rewards, sep='\\t')\n",
    "\n",
    "    obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "    \n",
    "    for step in range(max_size):\n",
    "        actions = {}\n",
    "        for _ in range(len(agents)):\n",
    "            agent = train_env.agent_selection\n",
    "            action = actors[agent].act(obs[agent], explore=True)\n",
    "            train_env.step(action)\n",
    "            actions[agent] = action\n",
    "\n",
    "        next_obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "        rewards = train_env.rewards\n",
    "        dones = train_env.dones\n",
    "        \n",
    "        data.append({'obs': obs, 'actions': actions, 'rewards': rewards})\n",
    "        \n",
    "        if len(data) == T or all(dones.values()):\n",
    "            for agent in agents:\n",
    "                actor = actors[agent]\n",
    "                critic = critics[agent]\n",
    "                \n",
    "                actions = np.array([d['actions'][agent] for d in data])\n",
    "                observations = np.array([d['obs'][agent] for d in data], np.float32)\n",
    "                rewards = np.array([d['rewards'][agent] for d in data], np.float32)\n",
    "                next_observations = next_obs[agent]\n",
    "\n",
    "                is_done = False\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    advantage = critic.advantage(observations, next_observations, rewards, is_done, discount)\n",
    "                    prob = actor.prob(observations, actions)\n",
    "\n",
    "                    critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "                    actor_loss = tf.reduce_mean(-tf.math.log(prob) * advantage)\n",
    "\n",
    "                actor.minimize(actor_loss, tape)\n",
    "                critic.minimize(critic_loss, tape)\n",
    "            \n",
    "                total_critic_loss += critic_loss.numpy()\n",
    "            data = []\n",
    "\n",
    "        if all(dones.values()):\n",
    "            train_env.reset()\n",
    "            obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "        else:\n",
    "            obs = next_obs\n",
    "\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            rewards = test_multiagent(actors, test_env, 5)\n",
    "            print(step, rewards, total_critic_loss / 1000, sep='\\t')\n",
    "            total_critic_loss = 0\n",
    "\n",
    "    return actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101ae401",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = simple_spread_v2.env()\n",
    "test_env = simple_spread_v2.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = train_agent_iaac(train_env, test_env, 50000, 25, 32, 0.001, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_caac(train_env, test_env, max_size, T, neurons, lr, discount):\n",
    "    train_env.reset()\n",
    "    agents = train_env.agents\n",
    "    actors = {\n",
    "        agent: DiscreteActor(train_env.observation_space(agent).shape, train_env.action_space(agent).n, neurons, lr)\n",
    "        for agent in agents\n",
    "    }\n",
    "    critic = Critic((sum(train_env.observation_space(agent).shape[0] for agent in agents),), neurons, lr)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    total_critic_loss = 0\n",
    "    \n",
    "    rewards = test_multiagent(actors, test_env, 5)\n",
    "    print(0, rewards, sep='\\t')\n",
    "\n",
    "    obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "    \n",
    "    for step in range(max_size):\n",
    "        actions = {}\n",
    "        for _ in range(len(agents)):\n",
    "            agent = train_env.agent_selection\n",
    "            action = actors[agent].act(obs[agent], explore=True)\n",
    "            train_env.step(action)\n",
    "            actions[agent] = action\n",
    "\n",
    "        next_obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "        rewards = train_env.rewards\n",
    "        dones = train_env.dones\n",
    "        \n",
    "        data.append({'obs': obs, 'actions': actions, 'rewards': rewards})\n",
    "        \n",
    "        if len(data) == T or all(dones.values()):\n",
    "            join_observations = np.array([\n",
    "                np.concatenate([d['obs'][agent] for agent in agents])\n",
    "                for d in data], np.float32\n",
    "            )\n",
    "            join_next_observations = np.concatenate([obs[agent] for agent in agents])\n",
    "            rewards = np.array([sum(d['rewards'][agent] for agent in agents) for d in data], np.float32)\n",
    "            is_done = False\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                advantage = critic.advantage(join_observations, join_next_observations, rewards, is_done, discount)\n",
    "                critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "\n",
    "            critic.minimize(critic_loss, tape)\n",
    "            total_critic_loss += critic_loss.numpy()\n",
    "            \n",
    "            for agent in agents:\n",
    "                actor = actors[agent]\n",
    "                \n",
    "                actions = np.array([d['actions'][agent] for d in data])\n",
    "                observations = np.array([d['obs'][agent] for d in data], np.float32)\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    prob = actor.prob(observations, actions)\n",
    "                    actor_loss = tf.reduce_mean(-tf.math.log(prob) * advantage)\n",
    "\n",
    "                actor.minimize(actor_loss, tape)\n",
    "    \n",
    "            data = []\n",
    "\n",
    "        if all(dones.values()):\n",
    "            train_env.reset()\n",
    "            obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "        else:\n",
    "            obs = next_obs\n",
    "\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            rewards = test_multiagent(actors, test_env, 5)\n",
    "            print(step, rewards, total_critic_loss / 1000, sep='\\t')\n",
    "            total_critic_loss = 0\n",
    "\n",
    "    return actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbe46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = train_agent_caac(train_env, test_env, 30000, 25, 32, 0.001, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multiagent(actors, env, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f535254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_maac(train_env, test_env, max_size, T, neurons, lr, discount, alpha, beta):\n",
    "    train_env.reset()\n",
    "    agents = train_env.agents\n",
    "    actors = {\n",
    "        agent: DiscreteActor(train_env.observation_space(agent).shape, train_env.action_space(agent).n, neurons, lr)\n",
    "        for agent in agents\n",
    "    }\n",
    "    critics = {\n",
    "        agent: Critic((\n",
    "            sum(train_env.observation_space(agent).shape[0] for agent in agents),\n",
    "        ), neurons, lr)\n",
    "        for agent in agents\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    total_critic_loss = 0\n",
    "    \n",
    "    rewards = test_multiagent(actors, test_env, 5)\n",
    "    print(0, rewards, sep='\\t')\n",
    "\n",
    "    obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "    \n",
    "    for step in range(max_size):\n",
    "        actions = {}\n",
    "        for _ in range(len(agents)):\n",
    "            agent = train_env.agent_selection\n",
    "            action = actors[agent].act(obs[agent], explore=True)\n",
    "            train_env.step(action)\n",
    "            actions[agent] = action\n",
    "\n",
    "        next_obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "        rewards = train_env.rewards\n",
    "        dones = train_env.dones\n",
    "        \n",
    "        data.append({'obs': obs, 'actions': actions, 'rewards': rewards})\n",
    "        \n",
    "        if len(data) == T or all(dones.values()):\n",
    "            for agent in agents:\n",
    "                weights = np.array([1 if a == agent else alpha for a in agents])\n",
    "                \n",
    "                actor = actors[agent]\n",
    "                critic = critics[agent]\n",
    "                \n",
    "                actions = np.array([d['actions'][agent] for d in data])\n",
    "                join_observations = np.array([\n",
    "                    np.concatenate([d['obs'][a] * w for a, w in zip(agents, weights)]) for d in data\n",
    "                ], np.float32)\n",
    "                observations = np.array([d['obs'][agent] for d in data], np.float32)\n",
    "\n",
    "                rewards = np.array([\n",
    "                    sum(d['rewards'][a] * w for a, w in zip(agents, weights)) for d in data\n",
    "                ], np.float32)\n",
    "\n",
    "                next_observations = np.concatenate([next_obs[a] * w for a, w in zip(agents, weights)])\n",
    "                \n",
    "                is_done = False\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    advantage = critic.advantage(join_observations, next_observations, rewards, is_done, discount)\n",
    "                    probs, prob = actor.probs_prob(observations, actions)\n",
    "\n",
    "                    critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "                    entropy_reward = beta * tf.reduce_sum(probs * tf.math.log(probs))\n",
    "                    actor_loss = tf.reduce_mean(-tf.math.log(prob) * advantage) + tf.reduce_mean(entropy_reward)\n",
    "\n",
    "                actor.minimize(actor_loss, tape)\n",
    "                critic.minimize(critic_loss, tape)\n",
    "            \n",
    "                total_critic_loss += critic_loss.numpy()\n",
    "            data = []\n",
    "\n",
    "        if all(dones.values()):\n",
    "            train_env.reset()\n",
    "            obs = {agent: train_env.observe(agent) for agent in agents}\n",
    "        else:\n",
    "            obs = next_obs\n",
    "\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            rewards = test_multiagent(actors, test_env, 5)\n",
    "            print(step, rewards, total_critic_loss / 1000, sep='\\t')\n",
    "            total_critic_loss = 0\n",
    "\n",
    "    return actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93a5542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-134.8897738898681\n",
      "999\t-129.13553212949284\t1.2905029108524322\n",
      "1999\t-128.98563809983168\t1.4688297519683837\n",
      "2999\t-143.09220432865473\t0.990796396613121\n",
      "3999\t-159.68138847413545\t0.9376717278957367\n",
      "4999\t-124.61196408413836\t0.5388937526494264\n",
      "5999\t-135.32937215010855\t0.4867990537881851\n",
      "6999\t-148.6854266184112\t0.4052819287488237\n",
      "7999\t-120.15791518577217\t0.1562084420444444\n",
      "8999\t-122.16176772873166\t0.06673483439628035\n",
      "9999\t-113.89447278377565\t0.08275051973224617\n",
      "10999\t-128.32040651694618\t0.06665869240323082\n",
      "11999\t-112.32564362998139\t0.07799159152293578\n",
      "12999\t-119.91842157797491\t0.09450711764302104\n",
      "13999\t-111.93774535607285\t0.05881499941088259\n",
      "14999\t-121.31306001617388\t0.05705398432444781\n",
      "15999\t-129.8663460574944\t0.036655323027167466\n",
      "16999\t-107.3982910533615\t0.05569755196385086\n",
      "17999\t-123.82905779740133\t0.03605788265122101\n",
      "18999\t-113.90873900679318\t0.03762549691088497\n",
      "19999\t-113.21296514046512\t0.037043443861417474\n",
      "20999\t-114.95416567078514\t0.044506456528324634\n",
      "21999\t-115.40773785526122\t0.04588381771137938\n",
      "22999\t-113.42252153345328\t0.03426661146397237\n",
      "23999\t-113.36336068575883\t0.04666134185239207\n",
      "24999\t-104.95179208506993\t0.0353735299448017\n",
      "25999\t-114.25636414775124\t0.03246063072653487\n",
      "26999\t-101.48879177129257\t0.020014420702587812\n",
      "27999\t-103.4550754352334\t0.03698335210583173\n",
      "28999\t-112.74448692911264\t0.035380450383992866\n",
      "29999\t-92.484095973179\t0.03875128244003281\n"
     ]
    }
   ],
   "source": [
    "actors = train_agent_maac(train_env, test_env, 30000, 25, 32, 0.001, 0.6, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15676f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-94.62816200230067"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_multiagent(actors, test_env, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d08b614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-168.55922004422385\n",
      "999\t-135.77024016232767\t1.7445443429946899\n",
      "1999\t-113.6593074224512\t1.2962639441490174\n",
      "2999\t-157.97933790876465\t0.9959262154698372\n",
      "3999\t-149.08666353911335\t0.8351412367224693\n",
      "4999\t-142.59356224381554\t0.6728708162903786\n",
      "5999\t-133.11243207695796\t0.36074586003646253\n",
      "6999\t-145.16797760445826\t0.30684090907638895\n",
      "7999\t-158.6709397320401\t0.13365346635971218\n",
      "8999\t-117.5462293117617\t0.14543445369135588\n",
      "9999\t-116.74002527252955\t0.09946269047586247\n",
      "10999\t-143.0268657272883\t0.05241031979955733\n",
      "11999\t-127.25677439194615\t0.09075423493329436\n",
      "12999\t-110.44828184685218\t0.06247028256300837\n",
      "13999\t-109.40661180612801\t0.06855511072138325\n",
      "14999\t-129.34704121854003\t0.08645762720936909\n",
      "15999\t-113.89474610017248\t0.06476798332203179\n",
      "16999\t-108.45280691605039\t0.08005596605082974\n",
      "17999\t-104.26223581172681\t0.1163931976344902\n",
      "18999\t-115.76644997196999\t0.06108395947841928\n",
      "19999\t-119.24109556268736\t0.05816329188272357\n",
      "20999\t-121.86490348728906\t0.056493203070713205\n",
      "21999\t-123.82078853419151\t0.07677385505044367\n",
      "22999\t-114.73587048508209\t0.06745046075060963\n",
      "23999\t-101.70386797472148\t0.05205412819422781\n",
      "24999\t-101.57116361301544\t0.03692751672398299\n",
      "25999\t-106.99481678964204\t0.047366800603922454\n",
      "26999\t-102.6503112972284\t0.060330439514480534\n",
      "27999\t-109.32313859306753\t0.06214715306297876\n",
      "28999\t-106.88228393671231\t0.04763721223268658\n",
      "29999\t-126.8247528645389\t0.035748233029386026\n",
      "30999\t-96.63895356650073\t0.04249225203448441\n",
      "31999\t-124.20646609648631\t0.059751108405180275\n",
      "32999\t-98.34175593035715\t0.06555449195019901\n",
      "33999\t-115.01377260075617\t0.04282047072669957\n",
      "34999\t-125.31001091451947\t0.045999351788079366\n",
      "35999\t-109.84881487842888\t0.056363436729414386\n",
      "36999\t-111.10898679027767\t0.06419983111624605\n",
      "37999\t-112.12235171350933\t0.04534085851418786\n",
      "38999\t-101.99740707348911\t0.051714142419048585\n",
      "39999\t-102.98348295252083\t0.05379602351086214\n",
      "40999\t-100.90148284319973\t0.05270516118174419\n",
      "41999\t-108.55291900254818\t0.048830150862806475\n",
      "42999\t-114.55356468748273\t0.07413763323507737\n",
      "43999\t-99.00586676729787\t0.04814443873416167\n",
      "44999\t-111.12603255029967\t0.0427537052189\n",
      "45999\t-110.3523676993733\t0.03133168828883209\n",
      "46999\t-116.81862840601939\t0.045820164777804165\n",
      "47999\t-126.78040578559796\t0.048425621771486474\n",
      "48999\t-103.31854204353215\t0.052044211502652614\n",
      "49999\t-116.7959269086484\t0.08784183400822804\n"
     ]
    }
   ],
   "source": [
    "actors = train_agent_maac(train_env, test_env, 50000, 25, 32, 0.001, 0.6, 0.5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1edb40f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-106.09781848248687"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_multiagent(actors, test_env, 5, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
