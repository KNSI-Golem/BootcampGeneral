{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przed spotkaniem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_hub\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upewnij się że poniższa komórka Ci się uruchamia przed spotkaniem\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "import tensorflow_hub as hub\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\").signatures[\"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 - Modele Sekwencyjne i ich zastosowania\n",
    "Modelem sekwencyjnym nazwiemy model, który jako wejście otrzymuje sekwencję, ale nie musi zwracać sekwencji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci rekurencyjne\n",
    "Zasadą działania sieci rekurencyjnej jest przechowywanie wyjścia poprzedniego elementu i wykorzystania go w kolejnym kroku.\n",
    "\n",
    "Dlaczego po prostu do sekwencji nie wykorzystać zwykłych gęstych sieci neuronowych?\n",
    "* Nie są w stanie przetwarzać sekwencji o różnych długościach\n",
    "* Biorą pod uwagę tylko aktualne dane wejściowe\n",
    "* Nie zapamiętują informacji o poprzednich danych wejściowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele sekwencyjne można podzielić na kilka przykładów\n",
    "* One-to-many, wejście o długości jednostkowej, wyjście o długości > 1. Przykład: generacja tekstu\n",
    "   \n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-many-ltr.png?d246c2f0d1e0f43a21a8bd95f579cb3b)\n",
    "\n",
    "* Many-to-one, przykład klasyfikacja sentymentu\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-one-ltr.png?c8a442b3ea9f4cb81f929c089b910c9d)\n",
    "\n",
    "* Many-to-many (tyle samo wejść co wyjść), przykład: NER(named entity recognition)\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232)\n",
    "\n",
    "* Many-to-many, przykład tłumaczenie maszynowe\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-different-ltr.png?8ca8bafd1eeac4e8c961d9293858407b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "Podstawową wersją sieci rekurencyjnej jest RNN(recurrent Neural Network)\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc)\n",
    "\\begin{align*}\n",
    "    &a^{<t>}=g_1(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)\\\\\n",
    "    &y^{<t>}=g_2(W_{ya}a^{<t>}+b_y)\n",
    "\\end{align*}\n",
    "Gdzie $W_{ax},W_{aa},W_{ya},b_a,b_y$ to wagi naszej sieci, a $g_1,g_2$ to funkcje aktywacji.\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png?74e25518f882f8758439bcb3637715e5)\n",
    "\n",
    "W klasycznym RNN $y^{<t>}=a^{<t>}$.\n",
    "\n",
    "Zalety:\n",
    "* Możliwość przetwarzania sekwencji o dowolnej długości\n",
    "* Rozmiar modelu nie rośnie razem z długością sekwencji\n",
    "* Bierze pod uwagę poprzednie stany\n",
    "\n",
    "Wady:\n",
    "* Wolne obliczenia\n",
    "* Problem wykorzystywania bardzo odległych stanów\n",
    "* Nie może brać pod uwagę przyszłych stanów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksplodujący/Zanikający gradient\n",
    "Problem ten często spotyka się podczas korzystania z RNN. Wynika to z tego, że podczas wyliczania gradientu przemnażamy przez siebie wielokrotnie gradienty dla danych chwil czasowych w związku z tym może on zacząć zanikać(jak przemnażamy małe wartości), albo \"wybuchnąć\"(jak przemnażamy duże wartości).\n",
    "\n",
    "Jak sobie poradzić z tym problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "Aby poradzić sobie z problemem zależności od bardzo odległych wyjść wprowadzono LSTM.\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)\n",
    "\n",
    "Gdzie $\\sigma$ to aktywacja sigmoid.\n",
    "\n",
    "Zatem jak można zauważyć LSTM posiada dwie \"ścieżki\" pamięci. Pierwszą jest tak zwany \"cell state\"\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)\n",
    "Ze względu na to, że ma on tylko liniowe interakcje łatwo jest o przepływ informacji tą scieżką. LSTM ma możliwość usuwania i dodawania informacji do tej ścieżki, co jest decydowanie przez tak zwane bramki(gates). Decydują one o tym czy dana informacją powinna dalej przejść\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png)\n",
    "\n",
    "Ponieważ sigmoida zwraca wartości między 0 a 1 decyduje jak \"dużo\" informacji powinno przepłynąć dalej.\n",
    "\n",
    "#### LSTM krok po kroku\n",
    "W pierwszym kroku decydujemy jak wiele aktualnej informacji powinno zostać w cell state.\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "Następnie decydujemy jak wiele nowej informacji powinniśmy dodać do cell state\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "Dokonujemy aktualizacji cell state\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
    "\n",
    "Na koniec wybieramy interesujące nas informacje z zakutalizowanego cell state, które zostaną zwrócone przez LSTM\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
    "\n",
    "Istnieją jeszcze inne warianty LSTM np. wykorzystujące cell state do bramek\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png)\n",
    "\n",
    "i takie, które wykorzystują jedną bramkę do zapominania/dodawania informacji do cell state\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU a LSTM\n",
    "GRU jest kolejną siecią rekurencyjną, której celem jest rozwiązanie odległych relacji między momentami czasu\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*yBXV9o5q7L_CvY7quJt3WQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "Czasem może nas interesować nie tylko infromacja z lewej do prawej ale także w drugą stronę, np. podczas klasyfikacji tekstu, w związku z tym aby otrzymać Biderctional RNN łączymy wyniki z dwóch sieci RNN, jedna \"czyta\" od lewej do prawej, a druga w drugą stronę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = tf.ones((16, 25, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=512,\n",
    "    activation=\"tanh\",\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False,\n",
    ")\n",
    "rnn_layer(example_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=512,\n",
    "    activation=\"tanh\",\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=True,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False,\n",
    ")\n",
    "rnn_layer(example_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=512,\n",
    "    activation=\"tanh\",\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False,\n",
    ")\n",
    "output =  rnn_layer(example_input)\n",
    "print(type(output))\n",
    "print(output[0].shape, output[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Klasyfikacja tekstu przy użyciu RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "for example, label in train_dataset.take(1):\n",
    "    print('texts: ', example.numpy()[:3])\n",
    "    print()\n",
    "    print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zadanie\n",
    "mając gotowe dane i tokenizer przetrenować sieć rekurencyjną do klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warstwe z embedingiem w tensorflow tworzymy następująco \n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True)\n",
    "# mask_zero powoduje, że wejścia które mają wartość 0 są pomijane w dalszych warstwach, tylko te warstwy muszą wspierwać maskowanie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aby stworzyć sieć RNN bidirectional wystarczy użyć\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    # tutaj wpisz swoje embeddingi i sieci RNN\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kompilacja modelu modelu\n",
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN a tekst\n",
    "\n",
    "Mając sekwencję możemy zamiast wykorzystywać RNN skorzystać z jednowymiarowej konwolucji. W tym przypadku rozmiar kernela(jądra) decyduje o tym na ile chwil czasowych jednocześnie patrzy CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie\n",
    "korzystając z warstwy `tf.keras.layers.Conv1D` i jakiejś warstwy poolingowej zastąpić w poprzednim modelu RNN siecią CNN.\n",
    "Argumenty są takie same jak dla konwolucji 2D tylko `kernel_size` i `strides` mogą być tylko liczbami całkowitymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rozwiązanie\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Tutaj dodaj CNN\n",
    "        mask_zero=False),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextual embeddings\n",
    "Przy użyciu sieci rekurencyjnych można tworzyć embeddingi, które uwzględniają kontekst w jakim użyte jest dane słowo. Istnieje wiele słów, które jak są wykorzystane bez kontekstu nie można jednoznacznie określić ich znaczenia(np. zamek jako budowla i jako zapięcie kurtki). W związku z tym wykorzystuje się modele sekwencyjne, które modelują słowo w zależności od jego \"otoczenia\"\n",
    "##### ELMo\n",
    "Model ELMo wykorzsytuje sieci Bidirectional RNN do reprezentacji kontekstu\n",
    "\n",
    "![](https://jalammar.github.io/images/Bert-language-modeling.png)\n",
    "\n",
    "Działanie ELMo\n",
    "\n",
    "![](https://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png)\n",
    "\n",
    "![](https://jalammar.github.io/images/elmo-embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\").signatures[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\"ELMo lives on sesame street.\"]\n",
    "\n",
    "# Extract ELMo features \n",
    "embeddings = elmo(tf.constant(x))[\"elmo\"]\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "Modele Seq2Seq składają się z dwóch modeli sekwencyjnych, zazwyczaj pierwszy nazywa się `encoder` a drugi `decoder`. Zazwyczaj sekwencje wejściowe i wyjściowe mogą być różnej długości.\n",
    "Przykłady zastosowań\n",
    "* Machine translation\n",
    "* Table summarization\n",
    "* Image captioning\n",
    "* Document Summarization\n",
    "* Question Answering(np. chatboty)\n",
    "* Speech recognition\n",
    "\n",
    "![](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mechanizm atencji\n",
    "\n",
    "W modelach Seq2Seq wykorzystuje się też często mechanizm atencji, który na podstawie stanu ukrytego ustala \"istotność\" danego wyjścia z encodera\n",
    "\n",
    "![](https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg)\n",
    "\n",
    "Istnieje wiele sposbów obliczania \"ważności\" danego wyjścia z encodera\n",
    "\n",
    "![](bahdanau_att.png)\n",
    "\n",
    "`Context vector` jest tworzony jako suma ważona na podstawie $a_t$, czyli $c_t=h_{enc}\\cdot a_t$. $v_a$ jest parametrem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie\n",
    "Zaimplementuj w tensorflow encoder, decoder oraz mechanizm atencji.\n",
    "Encoder ma zwracać wszystkie stany ukryte oraz ostatni, który potem jest pierwszy stanem ukrytym w dekoderze. `Context vector` ma być konkatenowany z wyjściami z decodera.\n",
    "\n",
    "* W atencji można wykorzystać warstwę `tf.keras.layers.AdditiveAttention`, która ma następujące wejścia\n",
    "  * inputs: List of the following tensors:\n",
    "    * query: Query Tensor of shape [batch_size, Tq, dim].\n",
    "    * value: Value Tensor of shape [batch_size, Tv, dim].\n",
    "    * key: Optional key Tensor of shape [batch_size, Tv, dim]. If not given, will use value for both key and value, which is the most common case.\n",
    "  * mask: List of the following tensors:\n",
    "    * query_mask: A boolean mask Tensor of shape [batch_size, Tq]. If given, the output will be zero at the positions where mask==False.\n",
    "    * value_mask: A boolean mask Tensor of shape [batch_size, Tv]. If given, will apply the mask such that values at positions where mask==False do not contribute to the result.\n",
    "  * training: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout).\n",
    "  * return_attention_scores: bool, it True, returns the attention scores (after masking and softmax) as an additional output argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        ...\n",
    "\n",
    "    def call(self, input_sequence):\n",
    "        ...\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        ...\n",
    "    \n",
    "    def call(self, encoder_output, decoder_hidden_states):\n",
    "        ...\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        ...\n",
    "\n",
    "    def call(self, hidden_states, context_vector):\n",
    "        ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers\n",
    "Transformery wywołały ogromny przełom w NLP, są one w stanie modelować relacje między dowolnie odległymi chwilami czasu i są szybsze ponieważ nie wymagają pętli(wystarcza tylko mnożenie wektorów i macierzy)\n",
    "\n",
    "![](https://deepfrench.gitlab.io/deep-learning-project/resources/transformer.png)\n",
    "\n",
    "Czym jest `Positional Encoding`? Istnieją dwie szkoły tworzenia\n",
    "* Uczymy embeddingi pozycji razem z modelem, czyli `Positional Encoding` jest parameterem modelu o ustalonej długości\n",
    "* Wykorzystujemy z góry zdefiniowaną funkcję np.\n",
    "\\begin{align*}\n",
    "  p_{i,j}=\n",
    "  \\begin{cases}\n",
    "    \\sin\\left(\\frac{i}{10000^{j/d}}\\right)\\\\\n",
    "    \\cos\\left(\\frac{i}{10000^{(j-1)/d}}\\right)\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "gdzie $i$-chwila czasu, $j$-j'ty wymiar w wektorze positional encodingu, $d$-wymiar positional encodingu.\n",
    "\n",
    "Positional Encoding jest potrzebny, żeby model był w stanie modelować dane wejściowe jako sekwencje, w przeciwnym wypadku, pozycja w której umieścimy token/wartość w chwili czasu nie ma żadnego znaczenia.\n",
    "\n",
    "Nakładamy maskę, żeby wyliczać atencję tylko na podstawie tokenów/chwil czasu, które chcemy, żeby model brał pod uwagę. Np. podczas uczenia generatora tekstu, będziemy maskowali wszystkie następne tokeny, ponieważ nie chcemy, żeby model genrował token na podstawie przyszłych tokenów, natomiast w przypadku klasyfikacji tekstu już taka maska nie jest potrzebna. Podczas trenowania nakłada też się zawsze maskę na tokeny odpowiadające paddingowi.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = tf.keras.layers.MultiHeadAttention(\n",
    "    num_heads=8,\n",
    "    key_dim=512,\n",
    "    value_dim=None,\n",
    "    dropout=0.0,\n",
    "    use_bias=True,\n",
    "    output_shape=None,\n",
    "    attention_axes=None,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    ")\n",
    "\n",
    "query = tf.ones((16, 25, 512))\n",
    "key = tf.ones((16, 35, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, att = mha(query=query, value=key, key=key, attention_mask=None, return_attention_scores=True, training=False)\n",
    "out.shape, att.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie\n",
    "Wykorzystując warstwę MultiHeadAttention zaimplementować blok encodera i decodera przedstawionego wcześniej na rysunku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        ...\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        ...\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        ...\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT\n",
    "BERT jest modelem językowym, który tworzy reprezentacje tekstu podanego mu na wejście. Jest on trenowany wykonując jednocześnie dwa zadania\n",
    "* Niektóre tokeny w zdaniach są zastępowane specjlanym tokenem `[MASK]` i jego zadaniem jest przewidzieć brakujące tokeny.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*ViwaI3Vvbnd-CJSQ.png)\n",
    "\n",
    "* Na wejściu otrzymuje dwa zdania oddzielone specjalnym tokenem `[SEP]` i ma przewidzieć czy zdania do siebie pasują. Na początku jest dodawany specjlany token `[CLS]`, z którego dokonywana jest ta predykcja. Pary pozytywne są tworzone przez branie zdań, które w zbiorze występują po sobie a negatywne przez złączenie dwóch losowych zdań w korpusie.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*m_kXt3uqZH9e7H4w.png)\n",
    "\n",
    "Ze względu na specyfikę zdania, jakie on wykonuje, wykorzystuje on tylko wartstwy `encoder` z wcześniej przedstawionego modelu Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Studying in PW is COOL\", return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografia\n",
    "#### Sieci rekurencyjne\n",
    "* [RNN i LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [ELMo-Embeddingi z kontekstem](https://arxiv.org/abs/1802.05365v2)\n",
    "* [BERT ELMo zilustrowane](https://jalammar.github.io/illustrated-bert/)\n",
    "* [Bahdanau Attention](https://arxiv.org/abs/1508.04025)\n",
    "#### Transformery\n",
    "* [Artykuł wprowadzający transformery](https://arxiv.org/abs/1706.03762)\n",
    "* [Wizualizacje działania transformerów](https://jalammar.github.io/illustrated-transformer/)\n",
    "* [BERT](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f60acc4feb66875ce3d3a7c45fd7be20da61778b03efde0a9ca4c6b3f9535b2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
