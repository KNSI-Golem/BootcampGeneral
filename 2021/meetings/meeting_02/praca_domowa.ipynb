{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b05d3a",
   "metadata": {},
   "source": [
    "<h1> Praca domowa po spotkaniu drugim </h1>\n",
    "W tym notebooku czeka Cię: \n",
    "<ul>\n",
    "    <li> Obrobienie datasetu, tak, żeby zawierał tylko potrzebne informacje </li>\n",
    "    <li> Implementacja RMS </li>\n",
    "    <li> Implementacja Gradient Descentu </li> \n",
    "    <li> Przejście przez regresję liniową </li>\n",
    "    <li> Implementacja normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc45fca",
   "metadata": {},
   "source": [
    "<h1> Getting started </h1>\n",
    "Wczytujemy dataset pomiarów pogody w Szeged (Węgry). Na podstawie różnych wartości będziemy próbowali przewidzieć jaka jest widoczność danego dnia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "#Wczytujemy dane \n",
    "weather_df = pd.read_csv (r'data/weatherHistory.csv')\n",
    "pd.DataFrame(weather_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24f88b",
   "metadata": {},
   "source": [
    "<h1> Zadanie 1: obróbka danych </h1> \n",
    "Najpierw potrzebujemy dobrze wybranych danych. Aby użyć regresji liniowej potrzebujemy danych numerycznych (czyli po prostu liczb). O danych kategorycznych (\"Partly Cloudy\", \"Sunny\", \"Cloudy\" itd.) będziemy rozmawiać na następnym spotkaniu. \n",
    "<ol> \n",
    "    <li> Usuń z datasetu kolumny, które nie są potrzebne. (Dane kategoryczne, może jakieś dane które nic nie wnoszą?) </li>\n",
    "    <li> Podziel dane na X i y. Targetem naszej regresji będzie visibility. </li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4021703",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### tutaj jest miejsce na twój kod ######\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268787c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizacja\n",
    "X=X.values\n",
    "max_values = np.max(X,axis=0)\n",
    "for i in range(0, X.shape[1]):    \n",
    "    X[:,i]=X[:,i]/max_values[i]\n",
    "    \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f347d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicjujemy bete na 0 \n",
    "beta = np.zeros(X.shape[1])\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ce284",
   "metadata": {},
   "source": [
    "<h1> Zadanie 2: Zaimplementuj funkcję straty </h1>\n",
    "<center><h2>${J} = \\frac{1}{n} \\sum \\limits _{i=1} ^{n} ({y}_{i} - pred_{i})^2$</h2></center>\n",
    "\n",
    "Ale żeby nie przeklejać z notebooka ze spotkania, parametry funkcji to y i pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf58e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y, pred_y):\n",
    "    ### tutaj miejsce na twój kod ####\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61912a",
   "metadata": {},
   "source": [
    "<h1> Zadanie 3: Zaimplementuj Gradient Descent </h1>\n",
    "<center><h2>$\\frac{\\partial{X}_i}{\\partial J} = \\frac{-2}{n} \\sum \\limits _{i=1} ^{n} (pred_{i})^{'} ({y}_{i} - pred_{i})^2$</h2></center>\n",
    "Pamiętaj, że funkcja straty ma inne parametry niż ta w notebooku ze spotkania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e599e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, beta, alpha, i):\n",
    "     # X - macierz X z danymi wejściowymi\n",
    "    # y - wektor z danymi wyjściowymi\n",
    "    # beta - wektor z przewidywanymi wartościami beta_0, beta_1 etc\n",
    "    # alpha - learning rate\n",
    "    # i - liczba iteracji którą będziemy wykonywać\n",
    "    \n",
    "    \n",
    "    #miejsce na kod\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return J, j, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liczymy wyniki\n",
    "J, j, beta = gradient_descent(X, y, beta, 0.5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patrzymy jak nasze wyniki różnią się od prawdzwiych - tutaj sample o rozmiarze 100 bo danych było tyle że nie zmieściły się na wykresie\n",
    "\n",
    "y_sample = y.sample(n= 100)\n",
    "y_hat = beta*X\n",
    "y_hat = np.sum(y_hat, axis=1)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x=list(range(0, 100)), y= y_sample, color='black')         \n",
    "plt.scatter(x=list(range(0, 100)), y=y_hat[y_sample.index], color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee8a20",
   "metadata": {},
   "source": [
    "<h1> Zadanie 4: Normal equation </h1>\n",
    "Nie zawsze musimy liczyć funkcję straty i spadek gradient, czasami całkiem dobre parametry da nam <b> normal equation</b>, czyli bezpośredni wzór na betę:\n",
    "<center><h2>$ \\beta = {(X^TX)}^{-1}(X^Ty)$</h2></center>\n",
    "Twoim zadaniem jest zaimplementowanie tego równania.\n",
    "\n",
    "Więcej o normal equation: https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d862046",
   "metadata": {},
   "outputs": [],
   "source": [
    "### miejsce na twój kod\n",
    "beta_normal = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18749d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#porównanie jakie wyniki daje nam spadek gradientu (czerwony) a normal equation (niebieski)\n",
    "\n",
    "y_hat = beta*X\n",
    "y_hat2 = beta_normal*X\n",
    "y_hat = np.sum(y_hat, axis=1)\n",
    "y_hat2 = np.sum(y_hat2, axis=1)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x=list(range(0, 100)), y= y_sample, color='black')         \n",
    "plt.scatter(x=list(range(0, 100)), y=y_hat[y_sample.index], color='red')\n",
    "plt.scatter(x=list(range(0, 100)), y=y_hat2[y_sample.index], color='blue')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
