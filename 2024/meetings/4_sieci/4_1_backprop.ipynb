{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotujmy dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Normalizacja\n",
    "X = X / 16.0\n",
    "\n",
    "y_one_hot = y.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(digits.images[i], cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {digits.target[i]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(X, y, batch_size):\n",
    "    # Shuffle the data\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X, y = X[indices], y[indices]\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield X[i:i + batch_size], y[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja straty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    batch_size = y_pred.shape[0]\n",
    "    \n",
    "    # Apply LogSoftmax (Numerically Stable)\n",
    "    log_probs = y_pred - np.log(np.sum(np.exp(y_pred), axis=1, keepdims=True)) #  - np.max(y_pred, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute the Negative Log-Likelihood (NLL) Loss\n",
    "    neg_log_likelihood = -np.mean(log_probs[np.arange(batch_size), y_true])\n",
    "    return neg_log_likelihood\n",
    "\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    batch_size = y_pred.shape[0]\n",
    "\n",
    "    # Softmax\n",
    "    exp_logits = np.exp(y_pred)  #  - np.max(y_pred, axis=1, keepdims=True)\n",
    "    softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    # Gradient of Cross-Entropy Loss\n",
    "    derivative = softmax\n",
    "    derivative[np.arange(batch_size), y_true] -= 1\n",
    "    derivative /= batch_size\n",
    "\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budujemy sieć neuronową"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaczynamy od hiperparametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 64\n",
    "hidden_dim = 64\n",
    "output_dim = 10\n",
    "learning_rate = 0.19\n",
    "epochs = 150\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wagi sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.5\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.5\n",
    "b2 = np.zeros((1, output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pętla trenująca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "n_batches = X_train.shape[0] // batch_size\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "        y_batch = y_batch.reshape(-1)\n",
    "        # Forward pass\n",
    "        Z1 = np.dot(X_batch, W1) + b1\n",
    "        A1 = np.tanh(Z1)  # Aktywacja\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "\n",
    "        # Wartość funkcji straty\n",
    "        loss = cross_entropy_loss(y_batch, Z2)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Wsteczna propagacja gradientu\n",
    "        dZ2 = cross_entropy_derivative(y_batch, Z2)  # Gradient wyjścia sieci\n",
    "        dW2 = np.dot(A1.T, dZ2) / X_batch.shape[0]\n",
    "        db2 = np.sum(dZ2, axis=0) / X_batch.shape[0]\n",
    "\n",
    "        dA1 = np.dot(dZ2, W2.T)\n",
    "        dZ1 = np.multiply(dA1, (1 - np.tanh(Z1) ** 2))  # Pochodna tanh\n",
    "        dW1 = np.dot(X_batch.T, dZ1) / X_batch.shape[0]\n",
    "        db1 = np.sum(dZ1, axis=0) / X_batch.shape[0]\n",
    "\n",
    "        # Optymalizacja wag (SGD)\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "\n",
    "    loss_history.append(epoch_loss / n_batches)\n",
    "    if epoch == 0 or (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss / n_batches:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs+1), loss_history)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_test = np.dot(X_test, W1) + b1\n",
    "A1_test = np.tanh(Z1_test)\n",
    "A2_test = np.dot(A1_test, W2) + b2\n",
    "\n",
    "test_loss = cross_entropy_loss(y_test, A2_test)\n",
    "predictions = np.argmax(A2_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test.reshape(-1))\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
