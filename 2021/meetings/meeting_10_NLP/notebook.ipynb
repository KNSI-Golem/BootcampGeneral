{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Czym w ogólne jest NLP?\n",
    "![Czym jest NLP](https://7wdata.be/wp-content/uploads/2021/09/DCF_NLP-in-the-Data-Center_ML-DL-Diagram.png)\n",
    "\n",
    "NLP - Natural Language Processing to dział AI zajmujący się przetwarzaniem języka naturalnego. Czyli takie rzeczy jak:\n",
    "* Generowanie artykułów\n",
    "* Podsumowywanie tekstu\n",
    "* POS (part of speach tagging)\n",
    "* Przetwarzanie zbiorów danych\n",
    "* Automatyczne Question anwsering\n",
    "itp... Ogólnie wszystko powiązanego z językami, którymi się posługują ludzie.\n",
    "Na codzień mamy doczynienia z tekstem więc nic dziwnego, że jest to tak istotna i prężnie rozwijająca się działka AI.\n",
    "W związku z tym że korzystamy z komputerów pojawiają się problemy jak reprezentować ten tekst, jak go przetwarzać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jak reprezentować tekst - podstawowe sposoby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoding\n",
    "Najprostszym sposobem jest już znany wam pewnie OneHotEncoding. Robimy to w następujący sposób:\n",
    "1) tworzymy słownik ze słowami zawierającymi się w naszym zbiorze $V=(słowo_1,słowo_2,\\dots,słowo_k)$\n",
    "2) reprezentujemy dany tekst na podstawie wektora 0 i 1, gdzie 0 dajemy jeżeli wystąpiło dane słowo\n",
    "czyli \n",
    "$$T = (a_1,a_2,\\dots,a_{|V|}) \\quad \\text{gdzie } a_i =\\begin{cases} 0 \\text{ gdy }słowo_i\\not\\in Tekst\\\\  1 \\text{ gdy }słowo_i\\in Tekst\\end{cases}$$\n",
    "Gdzie V to nasz słownik, a T to reprezentacja.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text):\n",
    "    words = text.split()\n",
    "    vocab = list(set(words))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def encode_text(text, vocab):\n",
    "    text_words = set(text.split())\n",
    "    return [word in text_words for word in vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([True, True, False], ['psa', 'ma', 'Ala'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Ala ma psa\"\n",
    "text2 = \"Maciek nie ma psa\"\n",
    "\n",
    "vocab = create_vocab(text)\n",
    "encode_text(text2, vocab), vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Wadą naszego porzedniego rozwiązania jest to, że tylko sprawdza czy słowo wystąpiło, dlatego teraz dodatkowo będziemy zliczali liczbę wystąpień słów.\n",
    "1) tworzymy słownik ze słowami zawierającymi się w naszym zbiorze $V=(słowo_1,słowo_2,\\dots,słowo_k)$\n",
    "2) reprezentujemy dany tekst na podstawie wektora wystąpień słów ze słownika w tekście\n",
    "czyli \n",
    "$$T = (a_1,a_2,\\dots,a_{|V|}) \\quad \\text{gdzie } a_i-\\text{liczba wystąpień }słowa_i\\text{ w tekście}$$\n",
    "Gdzie V to nasz słownik, a T to reprezentacja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(\n",
    "    subset=\"train\", shuffle=True, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer().fit(twenty_train.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': 56979,\n",
       " 'lerxst': 75358,\n",
       " 'wam': 123162,\n",
       " 'umd': 118280,\n",
       " 'edu': 50527,\n",
       " 'where': 124031,\n",
       " 'my': 85354,\n",
       " 'thing': 114688,\n",
       " 'subject': 111322,\n",
       " 'what': 123984,\n",
       " 'car': 37780,\n",
       " 'is': 68532,\n",
       " 'this': 114731,\n",
       " 'nntp': 87620,\n",
       " 'posting': 95162,\n",
       " 'host': 64095,\n",
       " 'rac3': 98949,\n",
       " 'organization': 90379,\n",
       " 'university': 118983,\n",
       " 'of': 89362,\n",
       " 'maryland': 79666,\n",
       " 'college': 40998,\n",
       " 'park': 92081,\n",
       " 'lines': 76032,\n",
       " '15': 4605,\n",
       " 'was': 123292,\n",
       " 'wondering': 124931,\n",
       " 'if': 65798,\n",
       " 'anyone': 28615,\n",
       " 'out': 90774,\n",
       " 'there': 114579,\n",
       " 'could': 42876,\n",
       " 'enlighten': 51793,\n",
       " 'me': 80638,\n",
       " 'on': 89860,\n",
       " 'saw': 104813,\n",
       " 'the': 114455,\n",
       " 'other': 90686,\n",
       " 'day': 45295,\n",
       " 'it': 68766,\n",
       " 'door': 48618,\n",
       " 'sports': 109581,\n",
       " 'looked': 76718,\n",
       " 'to': 115475,\n",
       " 'be': 32311,\n",
       " 'late': 74693,\n",
       " '60s': 16574,\n",
       " 'early': 50111,\n",
       " '70s': 18299,\n",
       " 'called': 37433,\n",
       " 'bricklin': 34995,\n",
       " 'doors': 48620,\n",
       " 'were': 123796,\n",
       " 'really': 99822,\n",
       " 'small': 108252,\n",
       " 'in': 66608,\n",
       " 'addition': 26073,\n",
       " 'front': 56989,\n",
       " 'bumper': 35612,\n",
       " 'separate': 106116,\n",
       " 'rest': 101378,\n",
       " 'body': 34181,\n",
       " 'all': 27436,\n",
       " 'know': 73201,\n",
       " 'can': 37565,\n",
       " 'tellme': 113986,\n",
       " 'model': 83256,\n",
       " 'name': 86001,\n",
       " 'engine': 51730,\n",
       " 'specs': 109271,\n",
       " 'years': 128026,\n",
       " 'production': 96144,\n",
       " 'made': 78784,\n",
       " 'history': 63363,\n",
       " 'or': 90252,\n",
       " 'whatever': 123989,\n",
       " 'info': 67156,\n",
       " 'you': 128402,\n",
       " 'have': 62221,\n",
       " 'funky': 57308,\n",
       " 'looking': 76722,\n",
       " 'please': 94362,\n",
       " 'mail': 78955,\n",
       " 'thanks': 114428,\n",
       " 'il': 66098,\n",
       " 'brought': 35187,\n",
       " 'by': 35983,\n",
       " 'your': 128420,\n",
       " 'neighborhood': 86580,\n",
       " 'guykuo': 60993,\n",
       " 'carson': 37960,\n",
       " 'washington': 123305,\n",
       " 'guy': 60990,\n",
       " 'kuo': 73823,\n",
       " 'si': 107206,\n",
       " 'clock': 40477,\n",
       " 'poll': 94825,\n",
       " 'final': 55386,\n",
       " 'call': 37423,\n",
       " 'summary': 111695,\n",
       " 'for': 56283,\n",
       " 'reports': 101054,\n",
       " 'keywords': 72384,\n",
       " 'acceleration': 25579,\n",
       " 'upgrade': 119484,\n",
       " 'article': 29573,\n",
       " 'shelley': 106745,\n",
       " '1qvfo9innc3s': 7797,\n",
       " '11': 2927,\n",
       " 'fair': 54214,\n",
       " 'number': 88363,\n",
       " 'brave': 34839,\n",
       " 'souls': 108955,\n",
       " 'who': 124147,\n",
       " 'upgraded': 119487,\n",
       " 'their': 114494,\n",
       " 'oscillator': 90586,\n",
       " 'shared': 106612,\n",
       " 'experiences': 53419,\n",
       " 'send': 106030,\n",
       " 'brief': 35013,\n",
       " 'message': 81263,\n",
       " 'detailing': 46702,\n",
       " 'with': 124616,\n",
       " 'procedure': 96085,\n",
       " 'top': 115701,\n",
       " 'speed': 109311,\n",
       " 'attained': 30214,\n",
       " 'cpu': 43146,\n",
       " 'rated': 99364,\n",
       " 'add': 26056,\n",
       " 'cards': 37840,\n",
       " 'and': 28146,\n",
       " 'adapters': 26035,\n",
       " 'heat': 62501,\n",
       " 'sinks': 107590,\n",
       " 'hour': 64153,\n",
       " 'usage': 119719,\n",
       " 'per': 92942,\n",
       " 'floppy': 55923,\n",
       " 'disk': 47721,\n",
       " 'functionality': 57269,\n",
       " '800': 20003,\n",
       " 'floppies': 55921,\n",
       " 'are': 29241,\n",
       " 'especially': 52522,\n",
       " 'requested': 101143,\n",
       " 'will': 124332,\n",
       " 'summarizing': 111694,\n",
       " 'next': 86977,\n",
       " 'two': 117211,\n",
       " 'days': 45311,\n",
       " 'so': 108558,\n",
       " 'network': 86752,\n",
       " 'knowledge': 73213,\n",
       " 'base': 31951,\n",
       " 'done': 48568,\n",
       " 'haven': 62224,\n",
       " 'answered': 28447,\n",
       " 'twillis': 117177,\n",
       " 'ec': 50247,\n",
       " 'ecn': 50337,\n",
       " 'purdue': 97049,\n",
       " 'thomas': 114755,\n",
       " 'willis': 124363,\n",
       " 'pb': 92486,\n",
       " 'questions': 98369,\n",
       " 'engineering': 51734,\n",
       " 'computer': 41614,\n",
       " 'distribution': 47982,\n",
       " 'usa': 119714,\n",
       " '36': 12197,\n",
       " 'well': 123759,\n",
       " 'folks': 56191,\n",
       " 'mac': 78586,\n",
       " 'plus': 94508,\n",
       " 'finally': 55392,\n",
       " 'gave': 58293,\n",
       " 'up': 119451,\n",
       " 'ghost': 58997,\n",
       " 'weekend': 123666,\n",
       " 'after': 26605,\n",
       " 'starting': 110333,\n",
       " 'life': 75828,\n",
       " 'as': 29620,\n",
       " '512k': 15032,\n",
       " 'way': 123422,\n",
       " 'back': 31414,\n",
       " '1985': 6449,\n",
       " 'sooo': 108874,\n",
       " 'market': 79520,\n",
       " 'new': 86864,\n",
       " 'machine': 78644,\n",
       " 'bit': 33527,\n",
       " 'sooner': 108872,\n",
       " 'than': 114418,\n",
       " 'intended': 67720,\n",
       " 'into': 68003,\n",
       " 'picking': 93765,\n",
       " 'powerbook': 95264,\n",
       " '160': 5023,\n",
       " 'maybe': 80010,\n",
       " '180': 5811,\n",
       " 'bunch': 35620,\n",
       " 'that': 114440,\n",
       " 'hopefully': 63965,\n",
       " 'somebody': 108801,\n",
       " 'answer': 28445,\n",
       " 'does': 48448,\n",
       " 'anybody': 28604,\n",
       " 'any': 28601,\n",
       " 'dirt': 47453,\n",
       " 'when': 124026,\n",
       " 'round': 103060,\n",
       " 'introductions': 68056,\n",
       " 'expected': 53383,\n",
       " 'heard': 62478,\n",
       " '185c': 6028,\n",
       " 'supposed': 111985,\n",
       " 'make': 79055,\n",
       " 'an': 28012,\n",
       " 'appearence': 28831,\n",
       " 'summer': 111697,\n",
       " 'but': 35805,\n",
       " 'anymore': 28613,\n",
       " 'since': 107539,\n",
       " 'don': 48546,\n",
       " 'access': 25605,\n",
       " 'macleak': 78702,\n",
       " 'had': 61546,\n",
       " 'more': 83706,\n",
       " 'has': 62123,\n",
       " 'rumors': 103499,\n",
       " 'about': 25399,\n",
       " 'price': 95882,\n",
       " 'drops': 49092,\n",
       " 'line': 76007,\n",
       " 'like': 75901,\n",
       " 'ones': 89897,\n",
       " 'duo': 49463,\n",
       " 'just': 71079,\n",
       " 'went': 123790,\n",
       " 'through': 114882,\n",
       " 'recently': 99966,\n",
       " 'impression': 66533,\n",
       " 'display': 47828,\n",
       " 'probably': 96047,\n",
       " 'swing': 112399,\n",
       " 'got': 59860,\n",
       " '80mb': 20119,\n",
       " 'rather': 99367,\n",
       " '120': 3412,\n",
       " 'feel': 54784,\n",
       " 'how': 64186,\n",
       " 'much': 84681,\n",
       " 'better': 32976,\n",
       " 'yea': 128018,\n",
       " 'looks': 76726,\n",
       " 'great': 60256,\n",
       " 'store': 110858,\n",
       " 'wow': 125133,\n",
       " 'good': 59779,\n",
       " 'solicit': 108738,\n",
       " 'some': 108799,\n",
       " 'opinions': 90097,\n",
       " 'people': 92923,\n",
       " 'use': 119737,\n",
       " 'its': 68857,\n",
       " 'worth': 125095,\n",
       " 'taking': 113289,\n",
       " 'size': 107705,\n",
       " 'money': 83495,\n",
       " 'hit': 63365,\n",
       " 'get': 58830,\n",
       " 'active': 25927,\n",
       " 'realize': 99814,\n",
       " 'real': 99791,\n",
       " 'subjective': 111325,\n",
       " 'question': 98356,\n",
       " 've': 120941,\n",
       " 'only': 89919,\n",
       " 'played': 94327,\n",
       " 'around': 29463,\n",
       " 'machines': 78652,\n",
       " 'breifly': 34918,\n",
       " 'figured': 55265,\n",
       " 'actually': 25953,\n",
       " 'uses': 119766,\n",
       " 'daily': 44889,\n",
       " 'might': 81998,\n",
       " 'prove': 96555,\n",
       " 'helpful': 62702,\n",
       " 'hellcats': 62665,\n",
       " 'perform': 92995,\n",
       " 'advance': 26298,\n",
       " 'email': 51268,\n",
       " 'll': 76377,\n",
       " 'post': 95136,\n",
       " 'news': 86914,\n",
       " 'reading': 99766,\n",
       " 'time': 115133,\n",
       " 'at': 30044,\n",
       " 'premium': 95676,\n",
       " 'finals': 55393,\n",
       " 'corner': 42684,\n",
       " 'tom': 115579,\n",
       " 'electrical': 51023,\n",
       " 'convictions': 42466,\n",
       " 'dangerous': 44992,\n",
       " 'enemies': 51670,\n",
       " 'truth': 116709,\n",
       " 'lies': 75818,\n",
       " 'nietzsche': 87254,\n",
       " 'jgreen': 70010,\n",
       " 'amber': 27774,\n",
       " 'joe': 70452,\n",
       " 'green': 60278,\n",
       " 're': 99721,\n",
       " 'weitek': 123734,\n",
       " 'p9000': 91542,\n",
       " 'harris': 62080,\n",
       " 'systems': 112687,\n",
       " 'division': 48072,\n",
       " '14': 4155,\n",
       " 'world': 125053,\n",
       " 'ssd': 109992,\n",
       " 'csd': 43791,\n",
       " 'com': 41105,\n",
       " 'newsreader': 86951,\n",
       " 'tin': 115192,\n",
       " 'version': 121234,\n",
       " 'pl9': 94200,\n",
       " 'robert': 102656,\n",
       " 'kyanko': 73948,\n",
       " 'rob': 102637,\n",
       " 'rjck': 102214,\n",
       " 'uucp': 119977,\n",
       " 'wrote': 125296,\n",
       " 'abraxis': 25422,\n",
       " 'iastate': 65446,\n",
       " 'writes': 125271,\n",
       " '734340159': 18618,\n",
       " 'class1': 40243,\n",
       " 'graphics': 60150,\n",
       " 'chip': 39415,\n",
       " 'far': 54383,\n",
       " 'low': 76894,\n",
       " 'level': 75431,\n",
       " 'stuff': 111196,\n",
       " 'goes': 59666,\n",
       " 'pretty': 95834,\n",
       " 'nice': 87170,\n",
       " 'quadrilateral': 98220,\n",
       " 'fill': 55347,\n",
       " 'command': 41190,\n",
       " 'requires': 101156,\n",
       " 'four': 56552,\n",
       " 'points': 94738,\n",
       " 'do': 48351,\n",
       " 'address': 26085,\n",
       " 'phone': 93582,\n",
       " 'information': 67186,\n",
       " 'corporation': 42714,\n",
       " 'scares': 104993,\n",
       " 'person': 93156,\n",
       " 'no': 87626,\n",
       " 'sense': 106065,\n",
       " 'humor': 64660,\n",
       " 'jonathan': 70557,\n",
       " 'winters': 124537,\n",
       " 'jcm': 69670,\n",
       " 'head': 62418,\n",
       " 'cfa': 38766,\n",
       " 'harvard': 62114,\n",
       " 'mcdowell': 80315,\n",
       " 'shuttle': 107159,\n",
       " 'launch': 74753,\n",
       " 'smithsonian': 108336,\n",
       " 'astrophysical': 30003,\n",
       " 'observatory': 89076,\n",
       " 'cambridge': 37504,\n",
       " 'ma': 78518,\n",
       " 'sci': 105245,\n",
       " '23': 9221,\n",
       " 'c5owcb': 36810,\n",
       " 'n3p': 85690,\n",
       " 'std': 110434,\n",
       " 'tombaker': 115597,\n",
       " 'baker': 31605,\n",
       " 'c5jlwx': 36662,\n",
       " '4h9': 14410,\n",
       " 'cs': 43740,\n",
       " 'cmu': 40658,\n",
       " 'etrat': 52721,\n",
       " 'ttacs1': 116829,\n",
       " 'ttu': 116864,\n",
       " 'pack': 91638,\n",
       " 'rat': 99356,\n",
       " 'clear': 40312,\n",
       " 'caution': 38203,\n",
       " 'warning': 123255,\n",
       " 'memory': 81066,\n",
       " 'verify': 121180,\n",
       " 'unexpected': 118750,\n",
       " 'errors': 52384,\n",
       " 'am': 27721,\n",
       " 'error': 52382,\n",
       " 'sorry': 108930,\n",
       " 'dumb': 49415,\n",
       " 'parity': 92080,\n",
       " 'previously': 95864,\n",
       " 'known': 73219,\n",
       " 'conditions': 41785,\n",
       " 'waivered': 123082,\n",
       " 'yes': 128084,\n",
       " 'we': 123575,\n",
       " 'already': 27613,\n",
       " 'knew': 73162,\n",
       " 'curious': 44098,\n",
       " 'meaning': 80696,\n",
       " 'quote': 98466,\n",
       " 'understanding': 118642,\n",
       " 'basically': 31994,\n",
       " 'bugs': 35510,\n",
       " 'system': 112674,\n",
       " 'software': 108677,\n",
       " 'things': 114692,\n",
       " 'checked': 39174,\n",
       " 'right': 101990,\n",
       " 'values': 120616,\n",
       " 'yet': 128096,\n",
       " 'because': 32422,\n",
       " 'they': 114646,\n",
       " 'aren': 29250,\n",
       " 'set': 106275,\n",
       " 'till': 115114,\n",
       " 'suchlike': 111534,\n",
       " 'fix': 55597,\n",
       " 'code': 40811,\n",
       " 'possibly': 95133,\n",
       " 'introduce': 68051,\n",
       " 'tell': 113980,\n",
       " 'crew': 43379,\n",
       " 'ok': 89592,\n",
       " 'see': 105818,\n",
       " '213': 8755,\n",
       " 'before': 32517,\n",
       " 'liftoff': 75855,\n",
       " 'ignore': 65894,\n",
       " 'dfo': 46923,\n",
       " 'vttoulu': 122323,\n",
       " 'tko': 115327,\n",
       " 'vtt': 122321,\n",
       " 'fi': 55154,\n",
       " 'foxvog': 56579,\n",
       " 'douglas': 48730,\n",
       " 'rewording': 101688,\n",
       " 'second': 105742,\n",
       " 'amendment': 27818,\n",
       " 'ideas': 65688,\n",
       " '58': 15598,\n",
       " '1r1eu1': 7862,\n",
       " '4t': 14706,\n",
       " 'transfer': 116179,\n",
       " 'stratus': 110966,\n",
       " 'cdt': 38493,\n",
       " 'sw': 112246,\n",
       " 'tavares': 113583,\n",
       " '1993apr20': 6517,\n",
       " '083057': 1416,\n",
       " '16899': 5341,\n",
       " 'ousrvr': 90771,\n",
       " 'oulu': 90757,\n",
       " '1qv87v': 7772,\n",
       " '4j3': 14445,\n",
       " 'c5n3gi': 36783,\n",
       " 'f8f': 53987,\n",
       " 'ulowell': 118213,\n",
       " 'jrutledg': 70815,\n",
       " 'john': 70487,\n",
       " 'lawrence': 74826,\n",
       " 'rutledge': 103598,\n",
       " 'massive': 79739,\n",
       " 'destructive': 46685,\n",
       " 'power': 95258,\n",
       " 'many': 79371,\n",
       " 'modern': 83291,\n",
       " 'weapons': 123603,\n",
       " 'makes': 79065,\n",
       " 'cost': 42835,\n",
       " 'accidental': 25622,\n",
       " 'crimial': 43405,\n",
       " 'these': 114625,\n",
       " 'mass': 79715,\n",
       " 'destruction': 46683,\n",
       " 'need': 86493,\n",
       " 'control': 42381,\n",
       " 'government': 59910,\n",
       " 'individual': 66931,\n",
       " 'would': 125110,\n",
       " 'result': 101426,\n",
       " 'needless': 86502,\n",
       " 'deaths': 45545,\n",
       " 'millions': 82144,\n",
       " 'keep': 72131,\n",
       " 'bear': 32357,\n",
       " 'non': 87728,\n",
       " 'existant': 53311,\n",
       " 'stating': 110379,\n",
       " 'coming': 41175,\n",
       " 'say': 104830,\n",
       " 'disagree': 47476,\n",
       " 'every': 52948,\n",
       " 'count': 42895,\n",
       " 'believe': 32651,\n",
       " 'individuals': 66938,\n",
       " 'should': 107022,\n",
       " 'own': 91190,\n",
       " 'find': 55411,\n",
       " 'hard': 61959,\n",
       " 'support': 111974,\n",
       " 'neighbor': 86579,\n",
       " 'nuclear': 88332,\n",
       " 'biological': 33438,\n",
       " 'nerve': 86663,\n",
       " 'gas': 58204,\n",
       " 'his': 63333,\n",
       " 'her': 62800,\n",
       " 'property': 96381,\n",
       " 'cannot': 37653,\n",
       " 'even': 52907,\n",
       " 'agree': 26747,\n",
       " 'keeping': 72135,\n",
       " 'hands': 61857,\n",
       " 'hope': 63962,\n",
       " 'us': 119701,\n",
       " 'sign': 107332,\n",
       " 'blank': 33723,\n",
       " 'checks': 39188,\n",
       " 'course': 42969,\n",
       " 'term': 114137,\n",
       " 'must': 85032,\n",
       " 'rigidly': 102018,\n",
       " 'defined': 45925,\n",
       " 'bill': 33331,\n",
       " 'doug': 48718,\n",
       " 'says': 104839,\n",
       " 'he': 62410,\n",
       " 'means': 80703,\n",
       " 'cbw': 38345,\n",
       " 'nukes': 88353,\n",
       " 'sarah': 104637,\n",
       " 'brady': 34759,\n",
       " 'she': 106682,\n",
       " 'street': 110999,\n",
       " 'sweeper': 112343,\n",
       " 'shotguns': 107016,\n",
       " 'semi': 105991,\n",
       " 'automatic': 30545,\n",
       " 'sks': 107901,\n",
       " 'rifles': 101978,\n",
       " 'doubt': 48704,\n",
       " 'using': 119781,\n",
       " 'allegedly': 27454,\n",
       " 'then': 114520,\n",
       " 'immediately': 66314,\n",
       " 'follows': 56205,\n",
       " 'thousands': 114818,\n",
       " 'killed': 72652,\n",
       " 'each': 50067,\n",
       " 'year': 128022,\n",
       " 'handguns': 61829,\n",
       " 'easily': 50148,\n",
       " 'reduced': 100241,\n",
       " 'putting': 97133,\n",
       " 'reasonable': 99867,\n",
       " 'restrictions': 101414,\n",
       " 'them': 114508,\n",
       " 'mean': 80693,\n",
       " 'read': 99755,\n",
       " 'presenting': 95741,\n",
       " 'first': 55525,\n",
       " 'argument': 29298,\n",
       " 'commonly': 41292,\n",
       " 'understood': 118652,\n",
       " 'switching': 112421,\n",
       " 'topics': 115712,\n",
       " 'point': 94725,\n",
       " 'evidently': 52979,\n",
       " 'show': 107045,\n",
       " 'not': 87949,\n",
       " 'allowed': 27520,\n",
       " 'later': 74699,\n",
       " 'analysis': 28075,\n",
       " 'given': 59199,\n",
       " 'consider': 42078,\n",
       " 'another': 28421,\n",
       " 'class': 40242,\n",
       " 'rocket': 102722,\n",
       " 'speak': 109205,\n",
       " 'company': 41355,\n",
       " 'vos': 122121,\n",
       " 'write': 125265,\n",
       " 'today': 115508,\n",
       " 'special': 109225,\n",
       " 'investors': 68150,\n",
       " 'packet': 91649,\n",
       " 'bmdelane': 34012,\n",
       " 'quads': 98226,\n",
       " 'uchicago': 117822,\n",
       " 'brian': 34971,\n",
       " 'manning': 79305,\n",
       " 'delaney': 46061,\n",
       " 'brain': 34772,\n",
       " 'tumor': 116953,\n",
       " 'treatment': 116382,\n",
       " 'reply': 101034,\n",
       " 'midway': 81978,\n",
       " 'chicago': 39339,\n",
       " '12': 3411,\n",
       " 'few': 55011,\n",
       " 'responded': 101346,\n",
       " 'request': 101142,\n",
       " 'astrocytomas': 29977,\n",
       " 'whom': 124164,\n",
       " 'couldn': 42878,\n",
       " 'thank': 114422,\n",
       " 'directly': 47438,\n",
       " 'bouncing': 34568,\n",
       " 'probs': 96074,\n",
       " 'sean': 105671,\n",
       " 'debra': 45570,\n",
       " 'sharon': 106627,\n",
       " 'thought': 114808,\n",
       " 'publicly': 96868,\n",
       " 'everyone': 52954,\n",
       " 'sure': 112031,\n",
       " 'glad': 59305,\n",
       " 'accidentally': 25623,\n",
       " 'rn': 102532,\n",
       " 'instead': 67583,\n",
       " 'rm': 102442,\n",
       " 'trying': 116727,\n",
       " 'delete': 46083,\n",
       " 'file': 55307,\n",
       " 'last': 74675,\n",
       " 'september': 106146,\n",
       " 'hmmm': 63542,\n",
       " 'bgrubb': 33081,\n",
       " 'dante': 45030,\n",
       " 'nmsu': 87589,\n",
       " 'grubb': 60545,\n",
       " 'ide': 65675,\n",
       " 'vs': 122259,\n",
       " 'scsi': 105492,\n",
       " 'mexico': 81419,\n",
       " 'state': 110355,\n",
       " 'las': 74650,\n",
       " 'cruces': 43599,\n",
       " 'nm': 87558,\n",
       " '44': 13679,\n",
       " 'dxb132': 49671,\n",
       " 'psuvm': 96737,\n",
       " 'psu': 96731,\n",
       " '1qlbrlinn7rk': 7597,\n",
       " 'dns1': 48346,\n",
       " 'pc': 92544,\n",
       " 'magazine': 78830,\n",
       " 'april': 29009,\n",
       " '27': 10013,\n",
       " '1993': 6475,\n",
       " '29': 10368,\n",
       " 'although': 27663,\n",
       " 'twice': 117168,\n",
       " 'fasst': 54465,\n",
       " 'esdi': 52481,\n",
       " '20': 8266,\n",
       " 'faster': 54474,\n",
       " 'devices': 46840,\n",
       " 'acceptance': 25596,\n",
       " 'long': 76683,\n",
       " 'been': 32491,\n",
       " 'stalled': 110196,\n",
       " 'incompatability': 66741,\n",
       " 'problems': 96071,\n",
       " 'installation': 67562,\n",
       " 'headaches': 62420,\n",
       " 'love': 76876,\n",
       " 'writers': 125270,\n",
       " 'stupid': 111219,\n",
       " 'statements': 110362,\n",
       " 'performance': 92999,\n",
       " 'those': 114800,\n",
       " 'numbers': 88371,\n",
       " 'list': 76145,\n",
       " 'actual': 25947,\n",
       " 'ranges': 99268,\n",
       " 'which': 124055,\n",
       " 'convince': 42475,\n",
       " 'such': 111533,\n",
       " 'statement': 110361,\n",
       " 'absurd': 25482,\n",
       " '5mb': 16105,\n",
       " 'ii': 65959,\n",
       " '40mb': 13429,\n",
       " '3mb': 13018,\n",
       " 'always': 27714,\n",
       " '25mb': 9853,\n",
       " 'standard': 110220,\n",
       " 'versions': 121236,\n",
       " 'shows': 107071,\n",
       " 'controler': 42384,\n",
       " 'range': 99261,\n",
       " 'indeed': 66843,\n",
       " 'controller': 42390,\n",
       " '6mb': 17800,\n",
       " '10mb': 2893,\n",
       " 'burst': 35747,\n",
       " 'note': 87961,\n",
       " 'increase': 66796,\n",
       " 'quadra': 98213,\n",
       " 'exist': 53308,\n",
       " 'too': 115663,\n",
       " 'mode': 83251,\n",
       " '16': 5022,\n",
       " 'wide': 124226,\n",
       " 'fast': 54467,\n",
       " '12mb': 3776,\n",
       " '20mb': 8627,\n",
       " '32': 11636,\n",
       " 'data': 45143,\n",
       " 'correct': 42724,\n",
       " 'reach': 99733,\n",
       " '96': 22194,\n",
       " 'facts': 54150,\n",
       " 'posted': 95150,\n",
       " 'newsgroup': 86932,\n",
       " 'ibm': 65480,\n",
       " 'sheet': 106713,\n",
       " 'available': 30616,\n",
       " 'ftp': 57132,\n",
       " 'sumex': 111680,\n",
       " 'aim': 26920,\n",
       " 'stanford': 110249,\n",
       " 'report': 101045,\n",
       " 'compare': 41363,\n",
       " 'txt': 117252,\n",
       " '173': 5581,\n",
       " '161': 5074,\n",
       " 'may': 80005,\n",
       " 'still': 110697,\n",
       " 'part': 92138,\n",
       " 'problem': 96064,\n",
       " 'both': 34523,\n",
       " 'inconsiant': 66764,\n",
       " 'though': 114806,\n",
       " 'documented': 48416,\n",
       " 'apple': 28856,\n",
       " 'salesperson': 104424,\n",
       " 'said': 104361,\n",
       " 'maximum': 79992,\n",
       " 'synchronous': 112584,\n",
       " 'ansynchronous': 28450,\n",
       " 'slower': 108174,\n",
       " 'seems': 105842,\n",
       " 'interface': 67802,\n",
       " 'think': 114696,\n",
       " 'driven': 49054,\n",
       " 'true': 116665,\n",
       " 'go': 59590,\n",
       " 'slam': 107976,\n",
       " 'understand': 118638,\n",
       " 'going': 59686,\n",
       " 'one': 89884,\n",
       " 'reference': 100314,\n",
       " 'digital': 47256,\n",
       " 'review': 101619,\n",
       " 'oct': 89215,\n",
       " '21': 8648,\n",
       " '1991': 6465,\n",
       " 'v8': 120402,\n",
       " 'n33': 85671,\n",
       " 'p8': 91509,\n",
       " 'holmes7000': 63775,\n",
       " 'iscsvax': 68565,\n",
       " 'uni': 118842,\n",
       " 'win': 124397,\n",
       " 'icon': 65583,\n",
       " 'help': 62696,\n",
       " 'northern': 87908,\n",
       " 'iowa': 68247,\n",
       " '10': 2336,\n",
       " 'downloaded': 48769,\n",
       " 'several': 106337,\n",
       " 'icons': 65593,\n",
       " 'bmp': 34038,\n",
       " 'figure': 55264,\n",
       " 'change': 38988,\n",
       " 'wallpaper': 123137,\n",
       " 'appreciated': 28930,\n",
       " 'thanx': 114436,\n",
       " 'brando': 34806,\n",
       " 'ps': 96643,\n",
       " 'kerr': 72278,\n",
       " 'ux1': 120128,\n",
       " 'cso': 43826,\n",
       " 'uiuc': 118089,\n",
       " 'stan': 110214,\n",
       " 'sigma': 107330,\n",
       " 'designs': 46587,\n",
       " 'double': 48689,\n",
       " 'c52u8x': 36456,\n",
       " 'b62': 31156,\n",
       " 'illinois': 66148,\n",
       " 'urbana': 119636,\n",
       " 'jap10': 69512,\n",
       " 'po': 94660,\n",
       " 'cwru': 44267,\n",
       " 'joseph': 70596,\n",
       " 'pellettiere': 92811,\n",
       " 'board': 34110,\n",
       " 'hardware': 61997,\n",
       " 'compression': 41559,\n",
       " 'works': 125042,\n",
       " 'autodoubler': 30524,\n",
       " 'also': 27618,\n",
       " 'over': 90946,\n",
       " 'work': 125017,\n",
       " 'diskdoubler': 47724,\n",
       " 'due': 49357,\n",
       " 'licensing': 75779,\n",
       " 'stac': 110133,\n",
       " 'technologies': 113811,\n",
       " 'owners': 91193,\n",
       " 'technology': 113812,\n",
       " 'writing': 125273,\n",
       " 'lost': 76825,\n",
       " 'wrong': 125288,\n",
       " 'being': 32596,\n",
       " 'whether': 124046,\n",
       " 'fault': 54528,\n",
       " 'something': 108821,\n",
       " 'else': 51215,\n",
       " 'however': 64200,\n",
       " 'decompress': 45718,\n",
       " 'troubled': 116637,\n",
       " 'recompress': 100068,\n",
       " 'without': 124640,\n",
       " 'usually': 119835,\n",
       " 'reappears': 99851,\n",
       " 'above': 25402,\n",
       " 'mentioned': 81133,\n",
       " 'freeware': 56834,\n",
       " 'expansion': 53371,\n",
       " 'utility': 119899,\n",
       " 'dd': 45444,\n",
       " 'expand': 53359,\n",
       " 'compressed': 41554,\n",
       " 'unless': 119030,\n",
       " 'installed': 67565,\n",
       " 'product': 96142,\n",
       " 'now': 88034,\n",
       " 'unlikely': 119035,\n",
       " 'holes': 63732,\n",
       " 'related': 100633,\n",
       " 'fixed': 55602,\n",
       " 'sad': 104273,\n",
       " 'very': 121265,\n",
       " 'reluctant': 100752,\n",
       " 'buy': 35855,\n",
       " 'stinky': 110726,\n",
       " 'hey': 62963,\n",
       " 'competition': 41449,\n",
       " 'computing': 41632,\n",
       " 'communications': 41316,\n",
       " 'services': 106253,\n",
       " 'office': 89410,\n",
       " '217': 8866,\n",
       " '333': 11796,\n",
       " '5217': 15110,\n",
       " 'stankerr': 110257,\n",
       " 'irwin': 68524,\n",
       " 'cmptrc': 40647,\n",
       " 'lonestar': 76681,\n",
       " 'org': 90364,\n",
       " 'arnstein': 29451,\n",
       " 'recommendation': 100059,\n",
       " 'duc': 49328,\n",
       " 'expires': 53441,\n",
       " 'sat': 104702,\n",
       " '05': 1049,\n",
       " '00': 0,\n",
       " 'gmt': 59534,\n",
       " 'computrac': 41633,\n",
       " 'inc': 66670,\n",
       " 'richardson': 101898,\n",
       " 'tx': 117230,\n",
       " 'ducati': 49331,\n",
       " 'gts': 60731,\n",
       " '13': 3802,\n",
       " '900gts': 21480,\n",
       " '1978': 6437,\n",
       " '17k': 5791,\n",
       " 'runs': 103528,\n",
       " 'paint': 91722,\n",
       " 'bronze': 35151,\n",
       " 'brown': 35194,\n",
       " 'orange': 90266,\n",
       " 'faded': 54163,\n",
       " 'leaks': 75033,\n",
       " 'oil': 89550,\n",
       " 'pops': 94986,\n",
       " '1st': 8042,\n",
       " 'accel': 25568,\n",
       " 'shop': 106965,\n",
       " 'trans': 116139,\n",
       " 'leak': 75028,\n",
       " 'sold': 108718,\n",
       " 'bike': 33301,\n",
       " 'owner': 91192,\n",
       " 'want': 123196,\n",
       " '3495': 11976,\n",
       " 'thinking': 114702,\n",
       " '3k': 12963,\n",
       " 'stable': 110130,\n",
       " 'mate': 79785,\n",
       " 'beemer': 32489,\n",
       " 'jap': 69511,\n",
       " 'myself': 85447,\n",
       " 'axis': 30868,\n",
       " 'motors': 83914,\n",
       " 'tuba': 116882,\n",
       " 'honk': 63910,\n",
       " 'therefore': 114586,\n",
       " 'dod': 48421,\n",
       " '0826': 1410,\n",
       " 'r75': 98828,\n",
       " 'david': 45240,\n",
       " 'terminus': 114156,\n",
       " 'ericsson': 52313,\n",
       " 'se': 105620,\n",
       " 'bold': 34252,\n",
       " 'popular': 94990,\n",
       " 'morality': 83685,\n",
       " 'camtec': 37563,\n",
       " 'electronics': 51051,\n",
       " 'leicester': 75240,\n",
       " 'england': 51741,\n",
       " '77': 19175,\n",
       " 'bangkok': 31744,\n",
       " '17570': 5691,\n",
       " 'freenet': 56826,\n",
       " 'carleton': 37881,\n",
       " 'ca': 37219,\n",
       " 'ad354': 25991,\n",
       " 'james': 69461,\n",
       " 'owens': 91180,\n",
       " 'previous': 95862,\n",
       " 'rude': 103421,\n",
       " 'hold': 63717,\n",
       " 'end': 51609,\n",
       " 'different': 47201,\n",
       " 'stick': 110671,\n",
       " 'look': 76715,\n",
       " 'again': 26646,\n",
       " 'intent': 67733,\n",
       " 'explaining': 53448,\n",
       " 'jung': 71028,\n",
       " 'moral': 83679,\n",
       " 'god': 59626,\n",
       " 'overlooked': 91031,\n",
       " 'main': 78995,\n",
       " 'seem': 105837,\n",
       " 'saying': 104834,\n",
       " 'unknowable': 119013,\n",
       " 'yep': 128076,\n",
       " 'jew': 69925,\n",
       " 'jewish': 69930,\n",
       " 'jews': 69935,\n",
       " 'covenant': 43007,\n",
       " 'between': 32988,\n",
       " 'yhwh': 128179,\n",
       " 'patriarchs': 92376,\n",
       " 'abraham': 25413,\n",
       " 'moses': 83810,\n",
       " 'case': 38019,\n",
       " 'establishes': 52575,\n",
       " 'follow': 56195,\n",
       " 'mankind': 79295,\n",
       " 'decide': 45644,\n",
       " 'boundaries': 34570,\n",
       " 'fall': 54272,\n",
       " 'sadducees': 104289,\n",
       " 'believed': 32652,\n",
       " 'torah': 115738,\n",
       " 'required': 101152,\n",
       " 'whereas': 124033,\n",
       " 'pharisees': 93451,\n",
       " 'ancestors': 28132,\n",
       " 'judaism': 70921,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = vectorizer.transform(twenty_train.data[:1])\n",
    "transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer().fit([\"Ala ma psa\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = vectorizer.transform(\n",
    "    [\"Maciek ma psa i ma wiele kotów, które nie lubią psów\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.toarray()\n",
    "# Zauważcie pierwszy problem, psów i psy mają tak naprawdę to samo znaczenie ale przez to\n",
    "# że są w innej odmianie nie są zliczane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency Inverse Document Frequency)\n",
    "Zauważmy, że wadą zliczania jest to, że mogą występować w naszym zbiorze teksty, w których często powtarza się\n",
    "to samo słowo i wtedy to że np. słowo \"paragraf\" wystąpiło 2137 za dużo nie znaczy. Z pomocą nadchodzi TF-IDF, który waży liczności\n",
    "w zależności od tego w ilu dokumentach dane słowo wystąpiło, czyli większe wagi chcemy przywiązywać słowom \"rozróżniającym\". Niech t oznacza token, d oznacza dokument oraz D-zbiór dokumentów wtedy\n",
    "\\begin{align*}\n",
    "    &tf(t,d)=\\frac{f_{t,d}}{\\sum_{t'\\in d} f_{t',d}}\\\\\n",
    "    &idf(t,D)=\\log\\frac{N}{|\\{d\\in D;\\,t\\in d\\}}\\\\\n",
    "    &tfidf(t,d,D)=tf(t,d)\\times idf(t,D)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(twenty_train.data)\n",
    "transformed_data = vectorizer.transform(twenty_train.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge!\n",
    "Korzystając z przedstawionych wcześniej reprezentacji tekstu przeucz drzewo losowe przewidujące klasę newsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset=\"test\", shuffle=True, random_state=42)\n",
    "train_text, train_class = twenty_train.data, twenty_train.target\n",
    "test_text, test_class = twenty_test.data, twenty_test.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozwiązanie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gramy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że czasem istotną informacją mogą być ciągi słów. Na tym właśnie polegają n-gramy. N-gram jest to ciąg n-słów.\n",
    "\n",
    "Przykład 2-grama: \\[\"Ala\", \"ma\"\\], \\[\"ma\", \"Psa\"\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lorem', 'ipsum', 'dolor'),\n",
       " ('ipsum', 'dolor', 'sit'),\n",
       " ('dolor', 'sit', 'amet,'),\n",
       " ('sit', 'amet,', 'consectetur'),\n",
       " ('amet,', 'consectetur', 'adipiscing'),\n",
       " ('consectetur', 'adipiscing', 'elit,'),\n",
       " ('adipiscing', 'elit,', 'sed'),\n",
       " ('elit,', 'sed', 'do'),\n",
       " ('sed', 'do', 'eiusmod'),\n",
       " ('do', 'eiusmod', 'tempor'),\n",
       " ('eiusmod', 'tempor', 'incididunt'),\n",
       " ('tempor', 'incididunt', 'ut'),\n",
       " ('incididunt', 'ut', 'labore'),\n",
       " ('ut', 'labore', 'et'),\n",
       " ('labore', 'et', 'dolore'),\n",
       " ('et', 'dolore', 'magna'),\n",
       " ('dolore', 'magna', 'aliqua.'),\n",
       " ('magna', 'aliqua.', 'Ut'),\n",
       " ('aliqua.', 'Ut', 'enim'),\n",
       " ('Ut', 'enim', 'ad'),\n",
       " ('enim', 'ad', 'minim'),\n",
       " ('ad', 'minim', 'veniam,'),\n",
       " ('minim', 'veniam,', 'quis'),\n",
       " ('veniam,', 'quis', 'nostrud')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funckja ngrams wymaga podania na wejście sekwencji\n",
    "list(ngrams(text.split(), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie wcześniej wymienione sposoby reprezentacji tekstu domyślnie operują na słowach ale mogą operować też na n-gramach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2)).fit([\"Ala ma psa\"])\n",
    "transformed = vectorizer.transform(\n",
    "    [\"Maciek ma psa i ma wiele kotów, które nie lubią psów\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchraba/anaconda3/envs/bootcamp/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ala', 'ala ma', 'ma', 'ma psa', 'psa']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 1, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie dla TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wracamy do poprzedniego zadania, spróbujcie poprawić wynik wykorzystując n-gramy, tylko nie przesadzajcie z n bo wam nie starczy RAMu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset=\"test\", shuffle=True, random_state=42)\n",
    "train_text, train_class = twenty_train.data, twenty_train.target\n",
    "test_text, test_class = twenty_test.data, twenty_test.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozwiązanie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zmniejszanie słownika\n",
    "### Stop words\n",
    "W każdym języku znajdują się słowa, które same nie niosą żadnej informacji, czasami warto pozbyć się takich słów, aby zmniejszyć wymiar słownika itp.\n",
    "\n",
    "Przykłady dla angielskiego: \"The\" \"who\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming i Lematyzacja\n",
    "\n",
    "Jak wcześniej zwróciłem uwagę czasami nie interesuje nas odmiana słowa tylko czy samo słowo wystąpiło. Np. chcemy w tekście szukać czy wystąpiła jakakolwiek odmiana słowa \"pies\" wtedy dokonujemy stemmingu(usunięcia ostatnich znaków ze słowa aby sprowadzić je do podstawowej formy) lub lematyzaji wykorzystującej kontekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mchraba/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mchraba/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses\n",
      "Lemma:caress Stemmed:caress\n",
      "flies\n",
      "Lemma:fly Stemmed:fli\n",
      "dies\n",
      "Lemma:dy Stemmed:die\n",
      "mules\n",
      "Lemma:mule Stemmed:mule\n",
      "denied\n",
      "Lemma:denied Stemmed:deni\n",
      "died\n",
      "Lemma:died Stemmed:die\n",
      "agreed\n",
      "Lemma:agreed Stemmed:agre\n",
      "owned\n",
      "Lemma:owned Stemmed:own\n",
      "humbled\n",
      "Lemma:humbled Stemmed:humbl\n",
      "sized\n",
      "Lemma:sized Stemmed:size\n",
      "meeting\n",
      "Lemma:meeting Stemmed:meet\n",
      "stating\n",
      "Lemma:stating Stemmed:state\n",
      "siezing\n",
      "Lemma:siezing Stemmed:siez\n",
      "itemization\n",
      "Lemma:itemization Stemmed:item\n",
      "sensational\n",
      "Lemma:sensational Stemmed:sensat\n",
      "traditional\n",
      "Lemma:traditional Stemmed:tradit\n",
      "reference\n",
      "Lemma:reference Stemmed:refer\n",
      "colonizer\n",
      "Lemma:colonizer Stemmed:colon\n",
      "plotted\n",
      "Lemma:plotted Stemmed:plot\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    \"caresses\",\n",
    "    \"flies\",\n",
    "    \"dies\",\n",
    "    \"mules\",\n",
    "    \"denied\",\n",
    "    \"died\",\n",
    "    \"agreed\",\n",
    "    \"owned\",\n",
    "    \"humbled\",\n",
    "    \"sized\",\n",
    "    \"meeting\",\n",
    "    \"stating\",\n",
    "    \"siezing\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"traditional\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "    \"plotted\",\n",
    "]\n",
    "for word in words:\n",
    "    lemmatized, stemmed = lemmatizer.lemmatize(word), stemmer.stem(word)\n",
    "    print(f\"{word}\\nLemma:{lemmatized} Stemmed:{stemmed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "regex czyli regular expressions to narzędzie pozwalające tworzyć wzorce tekstowe, które potem można wykorzystać np. do usuwania konkretnych wzorców z tekstu w celu filtorwania, wyciąganie emaili z tekstu itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tak się tworzy regexa\n",
    "pattern = re.compile(r\"@gmail.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@gmail.com']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.findall(\"kasjhfgasil jfalfhjas lfhaklfj pw@gmail.com\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak tworzyć regexy\n",
    "korzystając z cheat-sheetów\n",
    "\n",
    "![regex_cheat_sheet](https://media.cheatography.com/storage/thumb/davechild_regular-expressions.750.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteka re pozwala na wiele rzeczy [dokumentacja](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "funkcje:\n",
    "* __findall__: znajduje wszystkie wystąpienia wzorca\n",
    "* __search__: zwraca None albo pierwsze wystąpienie wzorca\n",
    "* __split__\n",
    "* __sub__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie: stworzyć regex który wyciąga emaile z tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"\")\n",
    "to_search = \"asoihfjaos hoasuf oajioah ofijoe pw@edu.pl klajshflakshfjklaj lfhasnoirhj weaoijpoa m abkcp@onet.pl\"\n",
    "pattern.findall(to_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pw', 'abkcp']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_with_group = r\"(\\w+)@[\\w\\.]+\"\n",
    "re.findall(regex_with_group, to_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pw', 'edu.pl'), ('abkcp', 'onet.pl')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_with_group = r\"(\\w+)@([\\w\\.]+)\"\n",
    "re.findall(regex_with_group, to_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asoihfjaos hoasuf oajioah ofijoe pw@uganda.ug klajshflakshfjklaj lfhasnoirhj weaoijpoa m abkcp@uganda.ug'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(regex_with_group, \"\\\\1@uganda.ug\", to_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asoihfjaos hoasuf oajioah ofijoe ale_jazda@edu.pl klajshflakshfjklaj lfhasnoirhj weaoijpoa m ale_jazda@onet.pl'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(regex_with_group, \"ale_jazda@\\\\2\", to_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pw', 'abkcp']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_with_group = r\"(\\w+)@(?:[\\w\\.]+)\"\n",
    "re.findall(regex_with_group, to_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizacja\n",
    "\n",
    "Poważnym problemem jest w jaki sposób tworzyć tokeny z tekstu:\n",
    "* znaki oddzielone spacjami?\n",
    "* znaki oddzielone na znakach interpunkcyjnych?\n",
    "* rozdzielanie na podstawie regexów?\n",
    "* inne metody?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oddzielanie spacjami\n",
    "jest to najprostszy sposób, ale też nie najlepszy, ponieważ np. słowo \"won't\" będzie jednym tokenem a samo \"n't\" można też dodać jako oddzielny token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oddzielanie na znakach interpunkcyjnych i spacjach\n",
    "Tutaj dodatkowo rozdzielamy spacją wszystkie znaki interpunkcyjne czyli np. \"Ala ma psa.\" -> \\[\"Ala\", \"ma\", \"psa\", \".\"\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oddzielanie na regexach\n",
    "jest to połączenie poprzednich metod i dodanie specjalnych fraz np. oddzielanie \"n't\" w języku angielskim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'did', \"n't\", 'want', 'to', 'come', '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I didn't want to come.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.',\n",
       " 'Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.',\n",
       " 'Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.',\n",
       " 'Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur?',\n",
       " 'Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subwords tokenization\n",
    "Wszystkie poprzednie metody mają problem z nowymi słowami, które mogą się pojawić podczas tokenizacji nowego tekstu. W przypadku poprzednich metod zastępuje się zazwyczaj słowa nie występujące w słowniku przez \"<unk>\". \n",
    "\n",
    "Kolejnym problemem jest wielkość słownika, im więcej słów chcemy posiadać tym większy musi być nasz słownik co prowadzi do coraz większych wymagań pamięciowych w celu operowania na tekstach. \n",
    "\n",
    "Problemy te są rozwiązywane przez tokenizatory, który dokonują podziału na tokeny, które nie są całymi słowami tylko ich fragmentami. W takich modelach z góry określa się wielkość słownika. Oczywiście powstaje pytanie jak wybierać ciągu znaków, które będą tokenami.\n",
    "\n",
    "Warto na nie zwrócić uwagę bo wszystkie aktualnie najlepsze modele językowe oparte o sieci neuronowe z nich korzystają ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'new', 'gp', '##u', '!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"I have a new GPU!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    vocab_size=2022,\n",
    "    min_frequency=1,\n",
    ")\n",
    "\n",
    "with open(\"tokenizer_train.txt\", \"w\") as f:\n",
    "    for line in twenty_train.data[:100]:\n",
    "        f.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train([\"tokenizer_train.txt\"], trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tim', 'e', 'to', 'to', 'k', 'en', 'ize', '!!', '!', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Time to tokenize!!!.\").tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można też dodawać normalizacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are u?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "normalizer.normalize_str(\"Héllò hôw are ü?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albo określać jak mają wyglądać dane wyjściowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Tim', 'e', 'to', 'to', 'k', 'en', 'ize', '!!', '!', '.', '[SEP]']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Time to tokenize!!!.\").tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z biblioteką [tokenizers](https://huggingface.co/docs/tokenizers/python/latest/) dodać wiele różnych rzeczy jak dodawanie specjalnych tokenów na początek koniec, wiele innych pretokenizatorów i wiele innych gotowych tokenizatorów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy i problemy lingwistyczne\n",
    "spacy jest biblioteką zawierającą wiele modeli do problemów lingwistycznych, które teraz sobie krótko omówimy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenizacja\n",
    "spacy także posiada tokenizacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akash\n",
      "has\n",
      "been\n",
      "buyed\n",
      "by\n",
      "byju\n",
      "'s\n",
      "in\n",
      "73,000\n",
      "Core\n",
      "'s\n"
     ]
    }
   ],
   "source": [
    "# wczytywanie modelu językowego\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Akash has been buyed by byju's in 73,000 Core's\")\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-Of_Speech (POS) - tagging\n",
    "problem ten polega na wyjaśnieniu w jaki sposób dane słowo jest wykorzystane w zdaniu. Ustalone jest 8 części mowy (po ang bo będzie łatwiej):\n",
    "* Noun\n",
    "* Pronoun\n",
    "* Adjective\n",
    "* Verb\n",
    "* Adverb\n",
    "* Preposition\n",
    "* Conjunction\n",
    "* Interjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP PRON pronoun, personal\n",
      "am VBP AUX verb, non-3rd person singular present\n",
      "Ritesh NNP PROPN noun, proper singular\n",
      ", , PUNCT punctuation mark, comma\n",
      "currently RB ADV adverb\n",
      "a DT DET determiner\n",
      "Computer NNP PROPN noun, proper singular\n",
      "Science NNP PROPN noun, proper singular\n",
      "and CC CCONJ conjunction, coordinating\n",
      "NLP NNP PROPN noun, proper singular\n",
      "Researcher NNP PROPN noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the token and its part-of-speech tag\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c0f3c15da9414a8dafdb18326f94be45-0\" class=\"displacy\" width=\"1800\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">am</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Ritesh,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">currently</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Computer</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Science</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">NLP</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">Researcher</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M380.0,441.5 L388.0,429.5 372.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,89.5 1620.0,89.5 1620.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-3\" stroke-width=\"2px\" d=\"M770,439.5 C770,177.0 1615.0,177.0 1615.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-4\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,264.5 1610.0,264.5 1610.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-6\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1255.0,441.5 L1263.0,429.5 1247.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-7\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0f3c15da9414a8dafdb18326f94be45-0-8\" stroke-width=\"2px\" d=\"M420,439.5 C420,2.0 1625.0,2.0 1625.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0f3c15da9414a8dafdb18326f94be45-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,441.5 L1633.0,429.5 1617.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n",
    "spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency Parsing\n",
    "Jest to proces tworzenia struktury gramatycznej zdania. Daje on nam zależność słów w zdaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> nsubj\n",
      "am --> ROOT\n",
      "Ritesh --> attr\n",
      ", --> punct\n",
      "currently --> advmod\n",
      "a --> det\n",
      "Computer --> nmod\n",
      "Science --> nmod\n",
      "and --> cc\n",
      "NLP --> compound\n",
      "Researcher --> appos\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.dep_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recogniton\n",
    "Czyli po prostu wykrywanie nazw własnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enitites: (Reliance, U.K., $7 billion)\n",
      "Reliance 0 8 ORG\n",
      "U.K. 30 34 GPE\n",
      "$7 billion 63 73 MONEY\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Reliance is looking at buying U.K. based analytics startup for $7 billion\"\n",
    ")\n",
    "# See the entity present\n",
    "print(f\"Enitites: {doc.ents}\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Recogniton\n",
    "nazywane też Entity Detection jest bardziej zaawansowany od NER, ponieważ rozpoznaje istotne elementy między innymi miejsca, ludzi, organizacje, języki itp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Amazon, 'ORG', 383),\n",
       " (English, 'LANGUAGE', 389),\n",
       " (Amazonia, 'GPE', 384),\n",
       " (Amazon, 'ORG', 383),\n",
       " (South America, 'LOC', 385),\n",
       " (7,000,000 km2, 'QUANTITY', 395),\n",
       " (2,700,000, 'CARDINAL', 397),\n",
       " (5,500,000 km2, 'CARDINAL', 397),\n",
       " (2,100,000, 'CARDINAL', 397),\n",
       " (nine, 'CARDINAL', 397),\n",
       " (Brazil, 'GPE', 384),\n",
       " (60%, 'PERCENT', 393),\n",
       " (Peru, 'GPE', 384),\n",
       " (13%, 'PERCENT', 393),\n",
       " (Colombia, 'GPE', 384),\n",
       " (10%, 'PERCENT', 393),\n",
       " (Bolivia, 'GPE', 384),\n",
       " (Ecuador, 'GPE', 384),\n",
       " (French, 'NORP', 381),\n",
       " (Guiana, 'PERSON', 380),\n",
       " (Guyana, 'GPE', 384),\n",
       " (Venezuela, 'GPE', 384),\n",
       " (Four, 'CARDINAL', 397),\n",
       " (Amazonas, 'WORK_OF_ART', 388),\n",
       " (one, 'CARDINAL', 397),\n",
       " (first, 'ORDINAL', 396),\n",
       " (France, 'GPE', 384),\n",
       " (Guiana Amazonian Park, 'WORK_OF_ART', 388),\n",
       " (Amazon, 'ORG', 383),\n",
       " (over half, 'CARDINAL', 397),\n",
       " (an estimated 390 billion, 'MONEY', 394),\n",
       " (16,000, 'CARDINAL', 397),\n",
       " (Amazon, 'ORG', 383),\n",
       " (Tapuyas, 'LOC', 385),\n",
       " (custom.[4, 'NORP', 381),\n",
       " (Amazonas, 'PERSON', 380),\n",
       " (Amazons, 'NORP', 381),\n",
       " (Greek, 'NORP', 381),\n",
       " (Herodotus, 'ORG', 383),\n",
       " (§, 'CARDINAL', 397),\n",
       " (Amazon, 'ORG', 383),\n",
       " (Amazon River, 'ORG', 383),\n",
       " (§ History, 'ORG', 383),\n",
       " (Amazonas, 'ORG', 383),\n",
       " (Jivaro, 'GPE', 384),\n",
       " (Jivaroan, 'NORP', 381),\n",
       " (Shuar, 'ORG', 383),\n",
       " (Brazil, 'GPE', 384),\n",
       " (Venezuela, 'GPE', 384),\n",
       " (Yanomami, 'PERSON', 380),\n",
       " (More than a third, 'CARDINAL', 397),\n",
       " (Yanomamo, 'NORP', 381)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    u\"\"\"The Amazon rainforest,[a] alternatively, the Amazon Jungle, also known in English as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations.\n",
    "\n",
    "The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, and Venezuela. Four nations have \"Amazonas\" as the name of one of their first-level administrative regions and France uses the name \"Guiana Amazonian Park\" for its rainforest protected area. The Amazon represents over half of the planet's remaining rainforests,[2] and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.[3]\n",
    "\n",
    "Etymology\n",
    "The name Amazon is said to arise from a war Francisco de Orellana fought with the Tapuyas and other tribes. The women of the tribe fought alongside the men, as was their custom.[4] Orellana derived the name Amazonas from the Amazons of Greek mythology, described by Herodotus and Diodorus.[4]\n",
    "\n",
    "History\n",
    "See also: History of South America § Amazon, and Amazon River § History\n",
    "Tribal societies are well capable of escalation to all-out wars between tribes. Thus, in the Amazonas, there was perpetual animosity between the neighboring tribes of the Jivaro. Several tribes of the Jivaroan group, including the Shuar, practised headhunting for trophies and headshrinking.[5] The accounts of missionaries to the area in the borderlands between Brazil and Venezuela have recounted constant infighting in the Yanomami tribes. More than a third of the Yanomamo males, on average, died from warfare.[6]\"\"\"\n",
    ")\n",
    "\n",
    "entities = [(i, i.label_, i.label) for i in doc.ents]\n",
    "entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " rainforest,[a] alternatively, the Amazon Jungle, also known in \n",
       "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    English\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LANGUAGE</span>\n",
       "</mark>\n",
       " as \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazonia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", is a moist broadleaf tropical rainforest in the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " biome that covers most of the Amazon basin of \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    South America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". This basin encompasses \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    7,000,000 km2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2,700,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " sq mi), of which \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    5,500,000 km2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2,100,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " sq mi) are covered by the rainforest. This region includes territory belonging to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nine\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " nations.</br></br>The majority of the forest is contained within \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brazil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    60%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " of the rainforest, followed by \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Peru\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    13%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Colombia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       ", and with minor amounts in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bolivia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ecuador\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    French\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Guiana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Guyana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", Suriname, and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Venezuela\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Four\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " nations have &quot;\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazonas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       "&quot; as the name of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of their \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       "-level administrative regions and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    France\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " uses the name &quot;\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Guiana Amazonian Park\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       "&quot; for its rainforest protected area. The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " represents \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    over half\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of the planet's remaining rainforests,[2] and comprises the largest and most biodiverse tract of tropical rainforest in the world, with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    an estimated 390 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " individual trees divided into \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    16,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " species.[3]</br></br>Etymology</br>The name \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is said to arise from a war Francisco de Orellana fought with the \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tapuyas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " and other tribes. The women of the tribe fought alongside the men, as was their \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    custom.[4\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       "] Orellana derived the name \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazonas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " from the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazons\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Greek\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " mythology, described by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Herodotus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and Diodorus.[4]</br></br>History</br>See also: History of South America \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    §\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon River\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    § History\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</br>Tribal societies are well capable of escalation to all-out wars between tribes. Thus, in the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazonas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", there was perpetual animosity between the neighboring tribes of the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jivaro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". Several tribes of the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jivaroan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " group, including the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Shuar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", practised headhunting for trophies and headshrinking.[5] The accounts of missionaries to the area in the borderlands between \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brazil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Venezuela\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " have recounted constant infighting in the \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Yanomami\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " tribes. \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    More than a third\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Yanomamo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " males, on average, died from warfare.[6]</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentacja słów i sieci neuronowe czyli embeddingi\n",
    "Embeddingi polegają na zmniejszeniu wymiaru danych tekstowych, aby zakodować ciąg słów o słowniku wielkości 50000 tworzymy macierz o wymiarach seq_len x 50000, co można się domyślić nie jest optymalne, embeddingi sprowadzają dane tekstowe do dużo niższego wymiaru np. 300.\n",
    "\n",
    "Przykładami embeddingów są:\n",
    "* Word2Vec\n",
    "* GloVe\n",
    "* FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "![Word2vec image](https://imgs.developpaper.com/imgs/1091794672-5c7cc60fab3ba_articlex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podobieństwo słów\n",
    "Mając embeddingi słów można badać ich podobieństwo badając ich odległość w przestrzeni, w której się znajdują. Najczęściej wykorzystuje się do tego odległość cosinusową\n",
    "\\begin{align*}\n",
    "    cos\\_sim(A,B) = \\frac{A\\cdot B}{||A||||B||}=cos(\\theta)\n",
    "\\end{align*}\n",
    "która pokazuje jaki jest kąt między dwoma wektorami, jeżeli 0 wtedy mamy 1 i oznacza to że wektory są tak samo skierowane czyli są podobne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'the', 'is', 'this', 'final', 'and', 'more', 'one', 'another', 'yet', 'second', 'word2vec', 'for', 'first']\n",
      "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"this\", \"is\", \"the\", \"first\", \"sentence\", \"for\", \"word2vec\"],\n",
    "    [\"this\", \"is\", \"the\", \"second\", \"sentence\"],\n",
    "    [\"yet\", \"another\", \"sentence\"],\n",
    "    [\"one\", \"more\", \"sentence\"],\n",
    "    [\"and\", \"the\", \"final\", \"sentence\"],\n",
    "]\n",
    "# size: (default 100) wymiar przestrzeni embeddingów.\n",
    "# window: (default 5) okno które będzie wykorzystywane do predykcji lub będzie predykowane.\n",
    "# min_count: (default 5) minimalna liczba wystąpień słowa aby było uwzględnione w słowniku.\n",
    "# workers: (default 3) liczba wątków wykorzystana do uczenia.\n",
    "# sg: (default 0 or CBOW) jaki algorytm ma być wykorzystany do uczenia 0-CBOW, 1-Skip-gram.\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "print(words)\n",
    "print(model.wv[\"sentence\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.wv[words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transformed = PCA(2).fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_transformed[:, 0], x_transformed[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(x_transformed[i, 0], x_transformed[i, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### wczytywanie gotowych embeddingów\n",
    "Teraz wczytam gotowe embeddingi, które można pobrać [tutaj](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filename = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(\n",
    "    positive=[\"woman\", \"king\"], negative=[\"man\"], topn=1\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddingi fasttext\n",
    "W Word2Vec tworzymy embeddingi słów w celu stworzenia embeddingu więc nie jesteśmy w stanie otrzymać embeddingu słowa spoza naszego słownika. Fasttext inaczej pochodzi do tworzenia embeddingów, ponieważ słowa, które są do siebie podobne(co do ogległości edycyjnej) powinny mieć podobne embeddingi postanowiono tworzyć je na podstawie n-gramów na znakach (dla n od 3 do 6). Jak to działa?\n",
    "1) Dodajemy na początek słowa '<' a na koniec '>'.\n",
    "\n",
    "![](https://amitness.com/images/fasttext-angular-brackets.png)\n",
    "\n",
    "2) tworzymy n-gramy dla słowa.\n",
    "3) Ponieważ liczba n-gramów może być ogromna dlatego, zamiast trenować embeddingi dla każdego unikatowego n-grama, trenowane jest B (B-bucket size). Każdy n-gram jest przetwarzany przy użyciu funkcji hashującej do liczby całkowitej między 1 a B.\n",
    "4) Do słownika dodajemy także słowa, które występują w zbiorze treningowym. Zatem mamy B+|V| embeddingów.\n",
    "\n",
    "#### Jak trenowany jest fasttext?\n",
    "\n",
    "Embeddingi są trenowane wykorzystując skip-gram z negatywnym próbkowaniem. Czyli na podstawie słowa chcemy przewidzieć słowa sąsiadujące. Ale embedding słowa na podstawie, którego chcemy przewidywać to suma n-gramów i embeddingu tego słowa.\n",
    "\n",
    "![](https://amitness.com/images/fasttext-negative-sampling-goal.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "Doc2Vec jest wykorzystaniem podobnego pomysłu co Word2Vec. Czyli na podstawie contekstu przewidujemy słowo. Ale skąd tutaj embedding dokumentu? Dodany jest dodatkowo embedding paragrafu jak na zdjęciu poniżej. Z dodatkiem tego embeddingu trenowany jest model  PV-DM(Distributed Memory version of Paragraph Vector) lub PV-DBOW(Words version of Paragraph Vector) (Podobny model do skip-gram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/486/0*x-gtU4UlO8FAsRvL.) ![](https://miro.medium.com/max/440/0*NtIsrbd4VQzUKVKr.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak zdobyć embedding nowego dokumentu? W tym celu zamrażane są wszystkie wagi sieci i jedyną zmienną jest embedding dokumentu, następnie ta zmienna jest aktualizowana trenując ją jak PV-DM lub PV-DBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możliwe też jest z wykorzystaniem Doc2Vec modelowanie gatunków. Zamiast unikatowego id można dodawać(dodatkowo lub jako jedyne wejście) tag związany z kategorią. W ten sposób otrzymamy embeddingi kategorii i po wyznaczeniu embeddingu danego dokumentu możemy powiedzieć do której kategorii on najprawdopodobniej należy.\n",
    "\n",
    "![](https://miro.medium.com/max/422/1*YfOv1_8tmTiahgbpEt5LCw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Tutaj możemy też podać kategorie\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[parametry klasy Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html#introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.infer_vector([\"My\", \"First\", \"Doc2Vec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7186111"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity_unseen_docs([\"human\", \"response\"], [\"computer\", \"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling i LDA(Latent Dirichlet allocation)\n",
    "\n",
    "LDA tworzy k tematów, do których przyporządkowuje nasze dokumenty wykorzystując rozkłady prawdopodobieństwa słów.\n",
    "LDA zakłada, że mamy model probabilistyczny, w którym mamy zmienną ukrytą T dotyczącą tematu, natomiast widzimy słowa i dokumenty. \n",
    "LDA działa w następujący sposób:\n",
    "* Przechodzi przez wszsytkie dokumenty i losowo przypisuje każde słowo do jednego z k tematów\n",
    "* Dla każdego dokumentu d przejdź przez wszystkie słowa w i zaktualizuj prawdopodobieństwo $p(w|t,d)=p(w|t)\\cdot p(t|d)$\n",
    "* Przejdź przez wyszystkie słowa i zaktualizuj temat do którego te słowa należą."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_texts = [\n",
    "    [\"computer\", \"time\", \"graph\"],\n",
    "    [\"survey\", \"response\", \"eps\"],\n",
    "    [\"human\", \"system\", \"computer\"],\n",
    "]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[0]\n",
    "vector = lda[unseen_doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.7749639),\n",
       " (1, 0.025000058),\n",
       " (2, 0.025000058),\n",
       " (3, 0.025000053),\n",
       " (4, 0.025014505),\n",
       " (5, 0.025010586),\n",
       " (6, 0.025000053),\n",
       " (7, 0.025010673),\n",
       " (8, 0.025000058),\n",
       " (9, 0.025000058)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza Sentymentu\n",
    "Analiza sentymentu polega na automatycznym wykrywaniu sentymentu na podstawie tekstu. Istnieje wiele \"sentymentów\" jakie można wygrywać, ale najpopularniejsze to:\n",
    "* Analiza sentymentu w skali 1-5 (1 bardzo negatywny, 5 bardzo pozytywny).\n",
    "* Analiza sentymentu pozytywnego lub negatywnego, np. wykrywanie negatywnych opinii produktu\n",
    "* Detekcja emocji np. smutek, szczęście, złość itp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Oceny filmów](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    \"movie_reviews/train.tsv.zip\", sep=\"\\t\", compression=\"zip\"\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    \"movie_reviews/test.tsv.zip\", sep=\"\\t\", compression=\"zip\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cel: stworzyć model, który najlepiej poradzi sobie z tym zadaniem\n",
    "Propozycje:\n",
    "* Doc2vec?\n",
    "* Embeddingi wsadzone do klasycznych modeli ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tematy\n",
    "### Sub-word tokenizers:\n",
    "* [Byte-Pair Encoding](https://arxiv.org/abs/1508.07909)\n",
    "* [WordPiece](https://ai.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html)\n",
    "* [Unigram Language Model](https://arxiv.org/pdf/1804.10959.pdf)\n",
    "* [SentencePiece](https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)\n",
    "### Embeddingi\n",
    "* [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "* [FastText](https://arxiv.org/pdf/1607.04606.pdf)\n",
    "* [Doc2Vec](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "### LDA\n",
    "[Dokładniejsze wyjaśnienie LDA](https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/)\n",
    "## Biblioteki:\n",
    "* [NLTK](https://www.nltk.org/)\n",
    "* [re](https://docs.python.org/3/library/re.html)\n",
    "* [Spacy](https://spacy.io/)\n",
    "* [FastText](https://fasttext.cc/)\n",
    "* [Hugging face tokenizers](https://huggingface.co/docs/tokenizers/python/latest/)\n",
    "* [Gensim](https://radimrehurek.com/gensim/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f60acc4feb66875ce3d3a7c45fd7be20da61778b03efde0a9ca4c6b3f9535b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('bootcamp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
